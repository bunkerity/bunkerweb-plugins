#!/usr/bin/env python3
"""
BunkerWeb Malware Scan - Hash Update Job

Scheduled task for periodic MalwareBazaar hash database updates.
This script is called by the BunkerWeb scheduler at configured intervals
and handles downloading, extracting, and importing malware hashes into Redis.

Features:
- Time-based update window (22-hour threshold) with success verification (retries immediately if last import failed)
- Intelligent change detection using ETag + file size
- Automatic retry with exponential backoff (3 retries)
- State persistence for bandwidth optimization (99.9% savings)
- Dual hash tables: malware_scan:hashes (full) and malware_scan:hashes:recent (recent)
- Bucketed key distribution (16 buckets per table for parallel import)
- Parallel import with ThreadPoolExecutor (3-6x faster)
- 10-second progress indicators with rate tracking
- Full table: infinite persistence (no TTL)
- Recent table: automatic 48-hour TTL (auto-expires old entries)
- Automatic processing of both full and recent files (scheduler controls frequency)
- Graceful error handling (continues on failures)
- SSL verification disabled for testing (temporary workaround)
- Automatic Redis connection management via Job class

Version: 0.7.44
Date: 2026-01-27
"""

from contextlib import suppress
from datetime import datetime, timezone
from os import getenv, sep
from os.path import join
from sys import exit as sys_exit, path as sys_path
from traceback import format_exc
from typing import Tuple, Optional
import os
import time
import zipfile
import tempfile
import shutil
import threading
import logging
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed

# ============================================================================
# EARLY LOGGER SETUP - Set up logging before any imports that might fail
# ============================================================================

# Check debug mode early
_is_debug = getenv("MALWARE_SCAN_DEBUG", "no") == "yes"

# Create basic Python logger that writes to stderr (goes to scheduler.log via Docker)
_early_logger = logging.getLogger("MALWARE-SCAN")
_early_logger.setLevel(logging.DEBUG if _is_debug else logging.INFO)
_handler = logging.StreamHandler()  # Writes to stderr by default
_handler.setFormatter(logging.Formatter(
    '[%(asctime)s] [%(name)s] [%(process)d] - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
))
_early_logger.addHandler(_handler)

# Add BunkerWeb dependencies to path
for deps_path in [join(sep, "usr", "share", "bunkerweb", *paths) for paths in (("deps", "python"), ("utils",), ("db",))]:
    if deps_path not in sys_path:
        sys_path.append(deps_path)

try:
    from requests import get, head
    from requests.exceptions import ConnectionError, Timeout
except ImportError:
    _early_logger.error("requests module not available")
    sys_exit(2)

try:
    import redis
except ImportError:
    _early_logger.error("redis module not available")
    sys_exit(2)

try:
    from logger import getLogger  # type: ignore
    from jobs import Job  # type: ignore
    # Use BunkerWeb logger
    LOGGER = getLogger("MALWARE-SCAN")
    _early_logger.info("Using BunkerWeb logger")
except ImportError:
    # Fallback: use the early logger we already set up
    _early_logger.warning("BunkerWeb logger not available, using fallback logger")
    LOGGER = _early_logger

    # Fallback Job class that doesn't require BunkerWeb
    class Job:
        def __init__(self, logger, filepath):
            self.logger = logger

        def cache_file(self, key, data):
            """Fallback cache_file - returns (False, error_message) as caching not available"""
            return False, "Caching not available in fallback mode"

        def get_cache(self, key):
            """Fallback get_cache - returns None as caching not available"""
            return None


VERSION = "0.8.0"
status = 0

# Debug mode detection
DEBUG_MODE = getenv("MALWARE_SCAN_DEBUG", "no") == "yes"

# Log script information
SCRIPT_PATH = os.path.abspath(__file__)
SCRIPT_NAME = os.path.basename(__file__)
LOGGER.info(f"[v{VERSION}] Script: {SCRIPT_NAME}")
LOGGER.info(f"[v{VERSION}] Path: {SCRIPT_PATH}")
if DEBUG_MODE:
    LOGGER.info(f"[v{VERSION}] DEBUG MODE ENABLED")
    LOGGER.debug(f"[v{VERSION}] Working directory: {os.getcwd()}")
    LOGGER.debug(f"[v{VERSION}] Python executable: {os.sys.executable}")
    LOGGER.debug(f"[v{VERSION}] Python version: {os.sys.version}")

# Initialize Job instance for Redis access
try:
    JOB = Job(LOGGER, __file__)
except Exception as e:
    LOGGER.error(f"[v{VERSION}] Failed to initialize Job: {e}")
    sys_exit(2)

# ============================================================================
# CHECKSUM VALIDATION (Security Enhancement)
# ============================================================================

def calculate_sha256(file_path: str, buffer_size: int = 65536) -> Optional[str]:
    """
    Calculate SHA256 checksum of a file.

    Uses buffered reading to handle large files efficiently.
    Prevents tampering detection and ensures download integrity.

    Args:
        file_path: Path to the file to checksum
        buffer_size: Buffer size for reading (default: 64KB)

    Returns:
        Hexadecimal SHA256 checksum string, or None on error
    """
    try:
        sha256_hash = hashlib.sha256()
        with open(file_path, "rb") as f:
            while True:
                data = f.read(buffer_size)
                if not data:
                    break
                sha256_hash.update(data)
        return sha256_hash.hexdigest()
    except Exception as e:
        LOGGER.error(f"[v{VERSION}] Failed to calculate SHA256 for {file_path}: {e}")
        return None

def verify_file_integrity(file_path: str, expected_size: Optional[int] = None) -> Tuple[bool, Optional[str], Optional[int]]:
    """
    Verify file integrity through size check and checksum calculation.

    Provides defense against:
    - Truncated downloads
    - Corrupted files
    - Tampering during transfer

    Args:
        file_path: Path to the file to verify
        expected_size: Expected file size in bytes (optional)

    Returns:
        Tuple of (is_valid, sha256_checksum, actual_size)
    """
    try:
        if not os.path.exists(file_path):
            LOGGER.error(f"[v{VERSION}] File does not exist: {file_path}")
            return False, None, None

        actual_size = os.path.getsize(file_path)

        # Size validation if expected size provided
        if expected_size is not None and actual_size != expected_size:
            LOGGER.error(f"[v{VERSION}] File size mismatch: expected {expected_size} bytes, got {actual_size} bytes")
            return False, None, actual_size

        # Calculate checksum
        checksum = calculate_sha256(file_path)
        if not checksum:
            return False, None, actual_size

        return True, checksum, actual_size

    except Exception as e:
        LOGGER.error(f"[v{VERSION}] File integrity check failed: {e}")
        return False, None, None

try:
    # Log environment variables in debug mode
    if DEBUG_MODE:
        LOGGER.debug(f"[v{VERSION}] Environment variables:")
        for key in sorted(os.environ.keys()):
            if key.startswith("MALWARE_SCAN_") or key.startswith("REDIS_"):
                value = os.environ[key]
                # Mask sensitive values (passwords, API keys, webhooks, secrets)
                sensitive_keywords = ["PASSWORD", "SECRET", "API_KEY", "TOKEN", "WEBHOOK"]
                if any(keyword in key for keyword in sensitive_keywords):
                    if value:
                        # Show partial value for debugging (first 10 chars + "...")
                        if len(value) > 10:
                            value = value[:10] + "...***MASKED***"
                        else:
                            value = "***MASKED***"
                    else:
                        value = "(empty)"
                LOGGER.debug(f"[v{VERSION}]   {key}={value}")

    # Add startup delay to prevent overlapping with cleanup job during scheduler restart
    LOGGER.info(f"[v{VERSION}] Waiting 20 seconds before starting update job to prevent job overlap...")
    time.sleep(20)
    LOGGER.info(f"[v{VERSION}] Starting update job")

    # Check if enabled
    if getenv("USE_MALWARE_SCANNER", "no") != "yes":
        LOGGER.info(f"[v{VERSION}] Malware scan disabled, skipping update")
        sys_exit(0)

    if getenv("MALWARE_SCAN_MALWAREBAZAAR_HASH_LOOKUP", "yes") != "yes":
        LOGGER.info(f"[v{VERSION}] MalwareBazaar hash lookup disabled, skipping update")
        sys_exit(0)

    if getenv("MALWARE_SCAN_MALWAREBAZAAR_AUTO_UPDATE", "yes") != "yes":
        LOGGER.info(f"[v{VERSION}] Auto-update disabled, skipping update")
        sys_exit(0)

    # Get Redis connection using BunkerWeb-configured environment variables
    try:
        # Check if using Redis Sentinel
        sentinel_hosts_str = getenv("REDIS_SENTINEL_HOSTS", "").strip()
        if DEBUG_MODE:
            LOGGER.debug(f"[v{VERSION}] REDIS_SENTINEL_HOSTS={repr(sentinel_hosts_str)}")

        if sentinel_hosts_str:
            # Redis Sentinel configuration
            sentinel_master = getenv("REDIS_SENTINEL_MASTER", "bw-master").strip()
            sentinel_username = getenv("REDIS_SENTINEL_USERNAME", "").strip() or None
            sentinel_password = getenv("REDIS_SENTINEL_PASSWORD", "").strip() or None
            redis_db_str = getenv("REDIS_DATABASE", "").strip() or "0"
            redis_username = getenv("REDIS_USERNAME", "").strip() or None
            redis_password = getenv("REDIS_PASSWORD", "").strip() or None
            redis_ssl = getenv("REDIS_SSL", "no").strip() == "yes"

            try:
                redis_db = int(redis_db_str)
            except ValueError as e:
                LOGGER.error(f"[v{VERSION}] Invalid Redis database: {e}")
                sys_exit(2)

            # Parse sentinel hosts (format: "host1:port1 host2:port2 ...")
            parsed_hosts = []
            for host_spec in sentinel_hosts_str.split():
                if ":" in host_spec:
                    h, p = host_spec.rsplit(":", 1)
                    try:
                        parsed_hosts.append((h, int(p)))
                    except ValueError:
                        LOGGER.warning(f"[v{VERSION}] Invalid sentinel host spec: {host_spec}")
                else:
                    parsed_hosts.append((host_spec, 26379))

            if not parsed_hosts:
                LOGGER.error(f"[v{VERSION}] Failed to parse Redis Sentinel hosts")
                sys_exit(2)

            if DEBUG_MODE:
                LOGGER.debug(f"[v{VERSION}] Using Redis Sentinel: master={sentinel_master}, hosts={parsed_hosts}")
                LOGGER.debug(f"[v{VERSION}] Sentinel config: username={sentinel_username is not None}, password={sentinel_password is not None}, ssl={redis_ssl}")
                LOGGER.debug(f"[v{VERSION}] Redis config: db={redis_db}, username={redis_username is not None}, password={redis_password is not None}, ssl={redis_ssl}")

            try:
                if DEBUG_MODE:
                    LOGGER.debug(f"[v{VERSION}] Creating Sentinel instance...")
                sentinel = redis.sentinel.Sentinel(
                    parsed_hosts,
                    username=sentinel_username,
                    password=sentinel_password,
                    sentinel_kwargs=dict(ssl=redis_ssl),
                    socket_connect_timeout=5,
                    socket_timeout=5
                )
                if DEBUG_MODE:
                    LOGGER.debug(f"[v{VERSION}] Sentinel instance created, getting connection...")
                redis_conn = sentinel.master_for(
                    sentinel_master,
                    db=redis_db,
                    username=redis_username,
                    password=redis_password,
                    ssl=redis_ssl
                )
                if DEBUG_MODE:
                    LOGGER.debug(f"[v{VERSION}] Got connection from sentinel.master_for")
            except Exception as e:
                LOGGER.error(f"[v{VERSION}] Sentinel connection error: {type(e).__name__}: {e}")
                raise
        else:
            # Standard Redis configuration
            if DEBUG_MODE:
                LOGGER.debug(f"[v{VERSION}] REDIS_HOST={repr(getenv('REDIS_HOST', ''))}")
                LOGGER.debug(f"[v{VERSION}] REDIS_PORT={repr(getenv('REDIS_PORT', ''))}")
                LOGGER.debug(f"[v{VERSION}] REDIS_DATABASE={repr(getenv('REDIS_DATABASE', ''))}")

            redis_host = getenv("REDIS_HOST", "").strip() or "127.0.0.1"
            redis_port_str = getenv("REDIS_PORT", "").strip() or "6379"
            redis_db_str = getenv("REDIS_DATABASE", "").strip() or "0"
            redis_ssl = getenv("REDIS_SSL", "no").strip() == "yes"
            redis_username = getenv("REDIS_USERNAME", "").strip() or None
            redis_password = getenv("REDIS_PASSWORD", "").strip() or None

            try:
                redis_port = int(redis_port_str)
                redis_db = int(redis_db_str)
            except ValueError as e:
                LOGGER.error(f"[v{VERSION}] Invalid Redis configuration: {e}")
                sys_exit(2)

            if DEBUG_MODE:
                LOGGER.debug(f"[v{VERSION}] Using standard Redis: host={redis_host}, port={redis_port}, db={redis_db}")
                LOGGER.debug(f"[v{VERSION}] Redis config: username={redis_username is not None}, password={redis_password is not None}, ssl={redis_ssl}")

            try:
                if DEBUG_MODE:
                    LOGGER.debug(f"[v{VERSION}] Creating Redis connection...")
                redis_conn = redis.Redis(
                    host=redis_host,
                    port=redis_port,
                    db=redis_db,
                    username=redis_username,
                    password=redis_password,
                    ssl=redis_ssl,
                    decode_responses=False,
                    socket_connect_timeout=5,
                    socket_timeout=5
                )
                if DEBUG_MODE:
                    LOGGER.debug(f"[v{VERSION}] Redis connection created")
            except Exception as e:
                LOGGER.error(f"[v{VERSION}] Redis creation error: {type(e).__name__}: {e}")
                raise

        try:
            if DEBUG_MODE:
                LOGGER.debug(f"[v{VERSION}] Pinging Redis...")
            redis_conn.ping()
            LOGGER.info(f"[v{VERSION}] Redis connected successfully")
        except Exception as ping_err:
            LOGGER.error(f"[v{VERSION}] Redis ping failed: {type(ping_err).__name__}: {ping_err}")
            raise
    except Exception as e:
        LOGGER.error(f"[v{VERSION}] Redis connection failed: {type(e).__name__}: {e}")
        if DEBUG_MODE:
            import traceback
            LOGGER.debug(f"[v{VERSION}] Traceback: {traceback.format_exc()}")
        sys_exit(2)

    # ============================================================================
    # PERSISTENT TEMP DIRECTORY MANAGEMENT
    # ============================================================================

    def get_or_create_temp_dir(redis_conn):
        """
        Get or create persistent temp directory for malware-scan downloads.

        Stores path in Redis to survive container restarts. If /var/cache/bunkerweb
        gets cleaned up, this function ensures we use a persistent /tmp directory.

        Returns:
            str: Path to the persistent temp directory
        """
        temp_dir_key = "malware_scan:scheduler:temp_dir"
        directory_missing = False
        old_dir_path = None

        try:
            # Try to get existing temp dir from Redis
            existing_dir = redis_conn.get(temp_dir_key)
            if existing_dir:
                existing_dir = existing_dir.decode()
                # Verify directory still exists
                if os.path.isdir(existing_dir):
                    if DEBUG_MODE:
                        LOGGER.debug(f"[v{VERSION}] Using existing temp directory from Redis: {existing_dir}")
                    # Refresh TTL to 24 hours
                    redis_conn.expire(temp_dir_key, 86400)
                    return existing_dir
                else:
                    # Directory from Redis no longer exists (likely after reboot)
                    directory_missing = True
                    old_dir_path = existing_dir
                    LOGGER.warning(f"[v{VERSION}] Temp directory from Redis no longer exists (likely after reboot): {existing_dir}")
                    # Delete stale Redis key
                    redis_conn.delete(temp_dir_key)
                    if DEBUG_MODE:
                        LOGGER.debug(f"[v{VERSION}] Deleted stale temp directory key from Redis")
        except Exception as e:
            LOGGER.warning(f"[v{VERSION}] Failed to get temp directory from Redis: {e}")

        # Create new temp directory in /tmp
        try:
            temp_dir = tempfile.mkdtemp(prefix='malware-scan-', dir='/tmp')
            if directory_missing:
                LOGGER.info(f"[v{VERSION}] ♻️  Created new temp directory after reboot: {temp_dir} (old: {old_dir_path})")
            else:
                LOGGER.info(f"[v{VERSION}] Created new persistent temp directory: {temp_dir}")

            # Store in Redis with 24-hour TTL
            redis_conn.set(temp_dir_key, temp_dir, ex=86400)
            if DEBUG_MODE:
                LOGGER.debug(f"[v{VERSION}] Stored temp directory path in Redis with 24h TTL")

            return temp_dir
        except Exception as e:
            LOGGER.error(f"[v{VERSION}] Failed to create temp directory: {e}")
            # Fallback to /tmp/malware-scan if all else fails
            fallback_dir = "/tmp/malware-scan"
            os.makedirs(fallback_dir, exist_ok=True)
            return fallback_dir

    # Get or create the persistent temp directory
    # NOTE: This directory stores downloaded ZIP files and metadata (survives across scheduler runs)
    # Separate extraction temp directories are created during import (cleaned up after each import)
    persistent_temp_dir = get_or_create_temp_dir(redis_conn)
    LOGGER.info(f"[v{VERSION}] Using persistent temp directory for downloads: {persistent_temp_dir}")

    # ============================================================================
    # FILE PATHS CONFIGURATION (using persistent temp directory)
    # ============================================================================

    # Always use persistent temp directory (ignoring env variable to avoid /var/cache/bunkerweb cleanup issues)
    csv_path = os.path.join(persistent_temp_dir, "hashes.csv")

    if DEBUG_MODE:
        LOGGER.debug(f"[v{VERSION}] Configuration:")
        LOGGER.debug(f"[v{VERSION}]   CSV path: {csv_path}")

    # Define both full and recent database files with configurable URLs
    full_url = getenv(
        "MALWARE_SCAN_MALWAREBAZAAR_UPDATE_URL_FULL",
        "https://bazaar.sysangels.ai/bunkerweb/malwarebazaar_full_split.zip"
    ).strip()
    recent_url = getenv(
        "MALWARE_SCAN_MALWAREBAZAAR_UPDATE_URL_RECENT",
        "https://bazaar.sysangels.ai/bunkerweb/malwarebazaar_recent_split.zip"
    ).strip()

    files_to_update = [
        {
            "name": "recent",
            "url": recent_url,
            "zip_file": csv_path.replace(".csv", "_recent.zip"),
            "state_key": "malware-scan:hashes:state:recent",
        },
        {
            "name": "full",
            "url": full_url,
            "zip_file": csv_path.replace(".csv", "_full.zip"),
            "state_key": "malware-scan:hashes:state:full",
        },
    ]

    if DEBUG_MODE:
        LOGGER.debug(f"[v{VERSION}] MalwareBazaar URLs:")
        LOGGER.debug(f"[v{VERSION}]   Full: {full_url}")
        LOGGER.debug(f"[v{VERSION}]   Recent: {recent_url}")

    # ============================================================================
    # CHECK CLEANUP VARIABLES (before init detection)
    # ============================================================================

    # Check if any cleanup flag is enabled
    cleanup_all = getenv("MALWARE_SCAN_CACHE_CLEANUP_ON_RESTART_ALL", "no") == "yes"
    cleanup_clamav = getenv("MALWARE_SCAN_CACHE_CLEANUP_ON_RESTART_CLAMAV", "no") == "yes"
    cleanup_virustotal = getenv("MALWARE_SCAN_CACHE_CLEANUP_ON_RESTART_VIRUSTOTAL", "no") == "yes"
    cleanup_sentinelone = getenv("MALWARE_SCAN_CACHE_CLEANUP_ON_RESTART_SENTINELONE", "no") == "yes"
    cleanup_malwarebazaar = getenv("MALWARE_SCAN_CACHE_CLEANUP_ON_RESTART_MALWAREBAZAAR", "no") == "yes"
    cleanup_attacker_ipv4 = getenv("MALWARE_SCAN_CACHE_CLEANUP_ON_RESTART_ATTACKER_IPV4", "no") == "yes"
    cleanup_attacker_ipv6 = getenv("MALWARE_SCAN_CACHE_CLEANUP_ON_RESTART_ATTACKER_IPV6", "no") == "yes"

    cleanup_requested = (
        cleanup_all or cleanup_clamav or cleanup_virustotal
        or cleanup_sentinelone or cleanup_malwarebazaar or cleanup_attacker_ipv4 or cleanup_attacker_ipv6
    )

    # ============================================================================
    # DETECT IF THIS IS FIRST RUN (based on Redis state keys)
    # ============================================================================

    # Check if state keys exist in Redis - if they don't, this is first run/init mode
    state_key_full = "malware-scan:hashes:state:full"
    state_key_recent = "malware-scan:hashes:state:recent"

    state_full_exists = redis_conn.exists(state_key_full)
    state_recent_exists = redis_conn.exists(state_key_recent)

    if DEBUG_MODE:
        LOGGER.debug(f"[v{VERSION}] Initialization detection:")
        LOGGER.debug(f"[v{VERSION}]   State key (full): {state_key_full}")
        LOGGER.debug(f"[v{VERSION}]   State key (recent): {state_key_recent}")
        LOGGER.debug(f"[v{VERSION}]   Full exists: {state_full_exists}")
        LOGGER.debug(f"[v{VERSION}]   Recent exists: {state_recent_exists}")
        # Try to get values if they exist
        if state_full_exists:
            try:
                full_value = redis_conn.get(state_key_full)
                LOGGER.debug(f"[v{VERSION}]   Full value: {full_value}")
            except Exception as e:
                LOGGER.debug(f"[v{VERSION}]   Failed to get full value: {e}")
        if state_recent_exists:
            try:
                recent_value = redis_conn.get(state_key_recent)
                LOGGER.debug(f"[v{VERSION}]   Recent value: {recent_value}")
            except Exception as e:
                LOGGER.debug(f"[v{VERSION}]   Failed to get recent value: {e}")

    # First run = neither state key exists in Redis
    is_init_run = not (state_full_exists and state_recent_exists)

    if DEBUG_MODE:
        LOGGER.debug(f"[v{VERSION}]   Is init run: {is_init_run}")
        LOGGER.debug(f"[v{VERSION}]   Cleanup requested: {cleanup_requested}")

    # Determine run mode
    if cleanup_requested:
        LOGGER.info(f"[v{VERSION}] [CLEANUP] Cache cleanup requested via environment variables")
    elif is_init_run:
        LOGGER.info(f"[v{VERSION}] [INIT] First run detected - initialization mode enabled")
    else:
        LOGGER.info(f"[v{VERSION}] [UPDATE] Periodic update run - no cleanup requested")

    # ============================================================================
    # CACHE CLEANUP (runs if cleanup requested OR first init run)
    # ============================================================================

    if cleanup_requested or is_init_run:
        if cleanup_requested:
            LOGGER.info(f"[v{VERSION}] [CLEANUP] Starting cache cleanup...")
        else:
            LOGGER.info(f"[v{VERSION}] [INIT] Starting cache cleanup before hash import...")

        # Build pattern list based on enabled flags
        patterns_to_delete = []
        if cleanup_all or cleanup_clamav:
            patterns_to_delete.append("plugin_malware_scan_clamav_*")
        if cleanup_all or cleanup_virustotal:
            patterns_to_delete.append("plugin_malware_scan_virustotal_*")
        if cleanup_all or cleanup_sentinelone:
            patterns_to_delete.append("plugin_malware_scan_sentinelone_*")
        # Clean when cleanup_all or cleanup_malwarebazaar is enabled
        if cleanup_all or cleanup_malwarebazaar:
            patterns_to_delete.append("plugin_malware_scan_malwarebazaar_*")
        if cleanup_all or cleanup_attacker_ipv4:
            patterns_to_delete.append("plugin_malware_scan_attacker_ipv4_*")
        if cleanup_all or cleanup_attacker_ipv6:
            patterns_to_delete.append("plugin_malware_scan_attacker_ipv6_*")

        # Always clean composite results cache
        if patterns_to_delete:
            patterns_to_delete.append("plugin_malware_scan_results_*")

        # Clean Redis cache
        if patterns_to_delete:
            LOGGER.info(f"[v{VERSION}] [CACHE_CLEANUP] Cleaning Redis patterns: {', '.join(patterns_to_delete)}")
            total_deleted = 0
            for pattern in patterns_to_delete:
                pattern_deleted = 0
                cursor = 0
                safe_prefix = "plugin_malware_scan_"

                # Validate pattern for safety
                if not pattern.startswith(safe_prefix):
                    LOGGER.error(f"[v{VERSION}] [CACHE_CLEANUP] Safety error: Pattern '{pattern}' invalid")
                    continue

                try:
                    while True:
                        cursor, keys = redis_conn.scan(cursor, match=pattern, count=100)
                        if keys:
                            pattern_deleted += redis_conn.unlink(*keys)
                        if cursor == 0:
                            break

                    if pattern_deleted > 0:
                        total_deleted += pattern_deleted
                        if DEBUG_MODE:
                            LOGGER.debug(f"[v{VERSION}] [CACHE_CLEANUP] Deleted {pattern_deleted} keys for pattern: {pattern}")
                except Exception as e:
                    LOGGER.warning(f"[v{VERSION}] [CACHE_CLEANUP] Error cleaning pattern '{pattern}': {e}")

            # Log cleanup results
            if total_deleted > 0:
                LOGGER.info(f"[v{VERSION}] [CACHE_CLEANUP] Redis cleanup complete - {total_deleted} keys deleted")
            else:
                LOGGER.info(f"[v{VERSION}] [CACHE_CLEANUP] Redis cleanup complete - 0 keys existing")

        # Cleanup MalwareBazaar hash databases and Redis buckets
        # Clean when cleanup_all or cleanup_malwarebazaar is enabled
        # NOTE: ZIP files are preserved to allow re-import without re-download
        if cleanup_all or cleanup_malwarebazaar:
            LOGGER.info(f"[v{VERSION}] [CACHE_CLEANUP] Cleaning MalwareBazaar Redis hash buckets...")
            mb_buckets_deleted = 0

            # Skip CSV, ZIP, and metadata file deletion to allow re-import from existing files
            LOGGER.info(f"[v{VERSION}] [CACHE_CLEANUP] Preserving ZIP files and metadata for re-import (no re-download needed)")

            # Delete all hash buckets unconditionally (will be re-imported from preserved ZIPs)
            bucket_prefixes = ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "A", "B", "C", "D", "E", "F"]

            # Delete full hash buckets
            for prefix in bucket_prefixes:
                bucket = f"malware_scan:hashes:{prefix}"
                try:
                    if redis_conn.delete(bucket):
                        mb_buckets_deleted += 1
                        if DEBUG_MODE:
                            LOGGER.debug(f"[v{VERSION}] [CACHE_CLEANUP] Deleted hash bucket: {bucket}")
                except Exception as e:
                    LOGGER.warning(f"[v{VERSION}] [CACHE_CLEANUP] Failed to delete bucket {bucket}: {e}")

            # Delete recent hash buckets
            for prefix in bucket_prefixes:
                bucket = f"malware_scan:hashes:recent:{prefix}"
                try:
                    if redis_conn.delete(bucket):
                        mb_buckets_deleted += 1
                        if DEBUG_MODE:
                            LOGGER.debug(f"[v{VERSION}] [CACHE_CLEANUP] Deleted hash bucket: {bucket}")
                except Exception as e:
                    LOGGER.warning(f"[v{VERSION}] [CACHE_CLEANUP] Failed to delete bucket {bucket}: {e}")

            LOGGER.info(f"[v{VERSION}] [CACHE_CLEANUP] Deleted {mb_buckets_deleted} hash buckets from Redis")

            # Clear BunkerWeb cache state files (will trigger re-import from preserved ZIPs)
            try:
                JOB.cache_file("malware-scan:hashes:state:full", b"")
                JOB.cache_file("malware-scan:hashes:state:recent", b"")
                LOGGER.info(f"[v{VERSION}] [CACHE_CLEANUP] Cleared state files (will trigger re-import from local ZIPs)")
            except Exception as e:
                LOGGER.warning(f"[v{VERSION}] [CACHE_CLEANUP] Failed to clear cache state files: {e}")

        LOGGER.info(f"[v{VERSION}] [CACHE_CLEANUP] ✅ Cache cleanup phase completed")

    # Lock key for preventing concurrent imports (defined here for closure access)
    lock_key = "malware-scan:hashes:import_lock"

    # ============================================================================
    # METADATA FILE FUNCTIONS (for HTTP header caching)
    # ============================================================================

    def save_zip_metadata(zip_path, last_modified=None, etag=None, content_length=None, sha256=None):
        """
        Save HTTP metadata for a ZIP file to a .meta file.
        This allows checking if remote file has changed without re-downloading.

        Args:
            zip_path: Path to the ZIP file
            last_modified: Last-Modified header from HTTP response
            etag: ETag header from HTTP response
            content_length: Content-Length header from HTTP response
            sha256: SHA256 checksum of the file
        """
        meta_path = zip_path + ".meta"
        try:
            metadata = {
                "last_modified": last_modified,
                "etag": etag,
                "content_length": content_length,
                "sha256": sha256,
                "saved_at": datetime.now(timezone.utc).isoformat()
            }
            with open(meta_path, 'w') as f:
                import json
                json.dump(metadata, f, indent=2)
            if DEBUG_MODE:
                LOGGER.debug(f"[v{VERSION}] Saved metadata to {meta_path}")
            return True
        except Exception as e:
            LOGGER.warning(f"[v{VERSION}] Failed to save metadata file: {e}")
            return False

    def load_zip_metadata(zip_path):
        """
        Load HTTP metadata for a ZIP file from its .meta file.

        Args:
            zip_path: Path to the ZIP file

        Returns:
            dict: Metadata dictionary or None if file doesn't exist or is invalid
        """
        meta_path = zip_path + ".meta"
        try:
            if os.path.exists(meta_path):
                with open(meta_path, 'r') as f:
                    import json
                    metadata = json.load(f)
                if DEBUG_MODE:
                    LOGGER.debug(f"[v{VERSION}] Loaded metadata from {meta_path}")
                return metadata
            return None
        except Exception as e:
            if DEBUG_MODE:
                LOGGER.debug(f"[v{VERSION}] Failed to load metadata file: {e}")
            return None

    # Define helper function to process one file
    def process_file(file_config, previous_import_state=None):
        """Download and import a single malware hash file (full or recent)

        Args:
            file_config: Configuration dict for the file to process
            previous_import_state: The import state from before this run started
        """
        file_name = file_config["name"]
        file_url = file_config["url"]
        zip_path = file_config["zip_file"]
        state_key = file_config["state_key"]
        file_status = 0

        try:
            # Load state from BunkerWeb cache
            state = {}
            cached_state = JOB.get_cache(state_key)
            if cached_state:
                try:
                    for line in cached_state.decode().split("\n"):
                        if line and "=" in line:
                            key, value = line.strip().split("=", 1)
                            state[key] = value.strip('"')
                    if DEBUG_MODE:
                        LOGGER.debug(f"[v{VERSION}] [{file_name}] Loaded state from cache: {len(state)} keys")
                        LOGGER.debug(f"[v{VERSION}] [{file_name}] State contents:")
                        for k, v in state.items():
                            LOGGER.debug(f"[v{VERSION}] [{file_name}]   {k}={v}")
                except Exception as e:
                    LOGGER.warning(f"[v{VERSION}] [{file_name}] Failed to parse cached state: {e}")
            else:
                if DEBUG_MODE:
                    LOGGER.debug(f"[v{VERSION}] [{file_name}] No cached state found")

            # Check time-based update window
            # - Full file: 22 hours (79200 seconds)
            # - Recent file: 5 minutes (300 seconds)
            # Only skip remote check if:
            # 1. Last import completed successfully (IMPORT_SUCCESS="yes" in state)
            # 2. Redis confirms last import was successful (import_state == "success")
            # 3. Less than the configured time window has passed since last download
            last_download_time = state.get("DOWNLOAD_TIME", "")
            import_success = state.get("IMPORT_SUCCESS", "")

            # Use the previous import state (from before this run started)
            redis_import_state = previous_import_state
            if DEBUG_MODE:
                LOGGER.debug(f"[v{VERSION}] [{file_name}] Previous Redis import state: {redis_import_state}")

            # Set update window based on file type
            is_recent = file_name == "recent"
            update_window_seconds = 300 if is_recent else 79200  # 5 minutes vs 22 hours
            update_window_display = "5m" if is_recent else "22h"

            if DEBUG_MODE:
                LOGGER.debug(f"[v{VERSION}] [{file_name}] Time window check:")
                LOGGER.debug(f"[v{VERSION}] [{file_name}]   last_download_time: {last_download_time}")
                LOGGER.debug(f"[v{VERSION}] [{file_name}]   import_success: {import_success}")
                LOGGER.debug(f"[v{VERSION}] [{file_name}]   redis_import_state: {redis_import_state}")
                LOGGER.debug(f"[v{VERSION}] [{file_name}]   update_window: {update_window_seconds}s ({update_window_display})")

            if last_download_time and import_success == "yes" and redis_import_state == "success":
                try:
                    last_download = datetime.fromisoformat(last_download_time)
                    now = datetime.now(timezone.utc)
                    elapsed_seconds = (now - last_download).total_seconds()
                    elapsed_hours = elapsed_seconds / 3600
                    elapsed_minutes = elapsed_seconds / 60

                    # Skip remote check if last download was recent AND successful AND cached file exists
                    if elapsed_seconds < update_window_seconds:
                        # Check if cached ZIP file still exists (may be deleted after temp dir recreation)
                        if not os.path.exists(zip_path):
                            LOGGER.warning(f"[v{VERSION}] [{file_name}] Cached file missing despite recent import - forcing re-download")
                            LOGGER.warning(f"[v{VERSION}] [{file_name}]   Expected location: {zip_path}")
                            LOGGER.warning(f"[v{VERSION}] [{file_name}]   (Likely temp directory changed after reboot)")
                        else:
                            if is_recent:
                                LOGGER.info(f"[v{VERSION}] [{file_name}] ✅ Last successful import was {elapsed_minutes:.1f}m ago (< {update_window_display} window), skipping remote check")
                            else:
                                LOGGER.info(f"[v{VERSION}] [{file_name}] ✅ Last successful import was {elapsed_hours:.1f}h ago (< {update_window_display} window), skipping remote check")
                            return 0
                    else:
                        if is_recent:
                            LOGGER.debug(f"[v{VERSION}] [{file_name}] Last successful import was {elapsed_minutes:.1f}m ago (>= {update_window_display} window), checking for updates")
                        else:
                            LOGGER.debug(f"[v{VERSION}] [{file_name}] Last successful import was {elapsed_hours:.1f}h ago (>= {update_window_display} window), checking for updates")
                except Exception as e:
                    LOGGER.debug(f"[v{VERSION}] [{file_name}] Failed to parse DOWNLOAD_TIME '{last_download_time}': {e}, proceeding with remote check")
            elif redis_import_state != "success":
                LOGGER.info(f"[v{VERSION}] [{file_name}] Last import did not complete successfully (Redis state: {redis_import_state}), forcing remote check to retry")
            elif import_success != "yes":
                LOGGER.info(f"[v{VERSION}] [{file_name}] Last import did not complete successfully (state marker missing), forcing remote check to retry")
            else:
                LOGGER.debug(f"[v{VERSION}] [{file_name}] No previous successful import found, proceeding with remote check")

            # Record import start time for this file type
            import time as time_module
            import_start_time_key = f"malware-scan:hashes:import_start_time:{file_name}"
            try:
                redis_conn.set(import_start_time_key, str(int(time_module.time())))
            except Exception as e:
                LOGGER.warning(f"[v{VERSION}] [{file_name}] Failed to set import start time: {e}")

            # Get remote metadata
            LOGGER.info(f"[v{VERSION}] [{file_name}] Checking remote file metadata...")
            max_retries = 3
            remote_etag = None
            remote_size = None
            remote_modified = None
            headers = {"User-Agent": f"bunkerweb - https://github.com/bunkerity/bunkerweb - malware-scan module v{VERSION}"}

            for attempt in range(1, max_retries + 1):
                try:
                    response = head(file_url, timeout=30, headers=headers)
                    if response.status_code == 200:
                        remote_etag = response.headers.get("etag", "").strip('"')
                        remote_size = response.headers.get("content-length", "")
                        if DEBUG_MODE:
                            LOGGER.debug(f"[v{VERSION}] [{file_name}] Remote: ETag={remote_etag}, Size={remote_size}")
                        break
                    else:
                        LOGGER.warning(f"[v{VERSION}] [{file_name}] HTTP {response.status_code}")
                except (ConnectionError, Timeout) as e:
                    LOGGER.warning(f"[v{VERSION}] [{file_name}] Network error: {e}")
                    if attempt < max_retries:
                        delay = 3 * attempt
                        LOGGER.info(f"[v{VERSION}] [{file_name}] Retrying in {delay}s (attempt {attempt}/{max_retries})...")
                        time.sleep(delay)
                except Exception as e:
                    LOGGER.error(f"[v{VERSION}] [{file_name}] Error checking metadata: {e}")

            if not remote_size or not remote_etag:
                LOGGER.error(f"[v{VERSION}] [{file_name}] Failed to get remote metadata after retries")
                return 2

            # Check if changed using ETag and Size
            saved_etag = state.get("SAVED_ETAG", "")
            saved_size = state.get("SAVED_SIZE", "")

            etag_match = saved_etag and saved_etag == remote_etag
            size_match = saved_size and saved_size == remote_size

            if DEBUG_MODE:
                LOGGER.debug(f"[v{VERSION}] [{file_name}] ETag comparison: saved='{saved_etag}' vs remote='{remote_etag}' (match: {etag_match})")
                LOGGER.debug(f"[v{VERSION}] [{file_name}] Size comparison: saved='{saved_size}' vs remote='{remote_size}' (match: {size_match})")

            # File is unchanged if both match AND last import was successful
            # If last import failed, force re-download even if file is unchanged
            if etag_match and size_match and import_success == "yes" and redis_import_state == "success":
                # Verify cached file integrity with checksum (tamper detection)
                if os.path.exists(zip_path):
                    saved_checksum = state.get("SAVED_SHA256", "")
                    if saved_checksum:
                        LOGGER.info(f"[v{VERSION}] [{file_name}] Verifying cached file integrity...")
                        is_valid, current_checksum, _ = verify_file_integrity(zip_path)
                        if is_valid and current_checksum == saved_checksum:
                            LOGGER.info(f"[v{VERSION}] [{file_name}] ✅ Cached file integrity verified (SHA256 match)")
                            LOGGER.info(f"[v{VERSION}] [{file_name}] ✅ File unchanged (ETag & Size match), skipping download")
                            return 0
                        elif is_valid and current_checksum != saved_checksum:
                            LOGGER.warning(f"[v{VERSION}] [{file_name}] ⚠️ Cached file checksum mismatch - file may have been tampered")
                            LOGGER.warning(f"[v{VERSION}] [{file_name}]   Expected: {saved_checksum[:16]}...{saved_checksum[-16:]}")
                            LOGGER.warning(f"[v{VERSION}] [{file_name}]   Got:      {current_checksum[:16]}...{current_checksum[-16:]}")
                            LOGGER.info(f"[v{VERSION}] [{file_name}] Forcing re-download for security")
                            # Continue to download section
                        else:
                            LOGGER.warning(f"[v{VERSION}] [{file_name}] Failed to verify cached file - forcing re-download")
                            # Continue to download section
                    else:
                        LOGGER.info(f"[v{VERSION}] [{file_name}] No saved checksum available - skipping integrity check")
                        LOGGER.info(f"[v{VERSION}] [{file_name}] ✅ File unchanged (ETag & Size match), skipping download")
                        return 0
                else:
                    LOGGER.warning(f"[v{VERSION}] [{file_name}] Cached file not found at {zip_path} - forcing re-download")
                    # Continue to download section
            elif etag_match and size_match:
                LOGGER.info(f"[v{VERSION}] [{file_name}] File unchanged but previous import failed (import_success={import_success}, redis_state={redis_import_state}), forcing re-download to retry")
                # Continue to download section

            if DEBUG_MODE:
                if not etag_match:
                    LOGGER.debug(f"[v{VERSION}] [{file_name}] ETag mismatch: '{saved_etag}' != '{remote_etag}'")
                if not size_match:
                    LOGGER.debug(f"[v{VERSION}] [{file_name}] Size mismatch: '{saved_size}' != '{remote_size}'")

            # Check if we have local metadata file (survives Redis cleanup)
            local_metadata = load_zip_metadata(zip_path) if os.path.exists(zip_path) else None
            if local_metadata and DEBUG_MODE:
                LOGGER.debug(f"[v{VERSION}] [{file_name}] Found local metadata: ETag={local_metadata.get('etag')}, Last-Modified={local_metadata.get('last_modified')}, SHA256={local_metadata.get('sha256', '')[:16]}...")

            # Optimization: Check remote SHA256 checksum before downloading ZIP
            # This avoids downloading large ZIP files if they haven't changed
            LOGGER.info(f"[v{VERSION}] [{file_name}] Checking remote SHA256 checksum...")
            sha256_url = file_url + ".sha256"
            published_checksum = None
            skip_download = False
            download_ok = False
            download_checksum = None
            remote_modified = None

            try:
                sha256_response = get(sha256_url, timeout=30, headers=headers)
                if sha256_response.status_code == 200:
                    # Parse checksum from .sha256 file
                    # Format is typically: "checksum  filename" or just "checksum"
                    published_checksum_line = sha256_response.text.strip()
                    published_checksum = published_checksum_line.split()[0].strip().lower()

                    if DEBUG_MODE:
                        LOGGER.debug(f"[v{VERSION}] [{file_name}] Published checksum: {published_checksum}")

                    # Validate checksum format (should be 64 hex chars)
                    if len(published_checksum) == 64 and all(c in '0123456789abcdef' for c in published_checksum):
                        LOGGER.info(f"[v{VERSION}] [{file_name}] Retrieved remote checksum: {published_checksum[:16]}...{published_checksum[-16:]}")

                        # If cached file exists, compare checksums
                        if os.path.exists(zip_path):
                            LOGGER.info(f"[v{VERSION}] [{file_name}] Calculating checksum of cached file...")
                            is_valid, cached_checksum, _ = verify_file_integrity(zip_path)

                            if is_valid and cached_checksum:
                                if DEBUG_MODE:
                                    LOGGER.debug(f"[v{VERSION}] [{file_name}] Cached checksum: {cached_checksum}")

                                # Compare checksums
                                if cached_checksum.lower() == published_checksum:
                                    LOGGER.info(f"[v{VERSION}] [{file_name}] ✅ Cached file matches remote checksum - skipping download")
                                    LOGGER.info(f"[v{VERSION}] [{file_name}] Using cached file: {zip_path}")

                                    # Update state with current metadata (but don't re-import)
                                    state_data = f'SAVED_ETAG="{remote_etag}"\nSAVED_SIZE="{remote_size}"\nSAVED_SHA256="{cached_checksum}"\nDOWNLOAD_TIME="{datetime.now(timezone.utc).isoformat()}"\nIMPORT_SUCCESS="yes"\n'
                                    cached, err = JOB.cache_file(state_key, state_data.encode())
                                    if not cached:
                                        LOGGER.warning(f"[v{VERSION}] [{file_name}] Failed to update cache state: {err}")

                                    # Also save/update local metadata file
                                    save_zip_metadata(
                                        zip_path,
                                        last_modified=None,  # Don't have Last-Modified from SHA256 check
                                        etag=remote_etag,
                                        content_length=remote_size,
                                        sha256=cached_checksum
                                    )

                                    # Proceed with import using cached file
                                    download_ok = True
                                    download_checksum = cached_checksum
                                    remote_modified = datetime.now(timezone.utc).strftime("%a, %d %b %Y %H:%M:%S GMT")
                                    # Skip to import section by setting flag and jumping past download
                                    skip_download = True
                                else:
                                    LOGGER.info(f"[v{VERSION}] [{file_name}] Cached file checksum differs - downloading new version")
                                    if DEBUG_MODE:
                                        LOGGER.debug(f"[v{VERSION}] [{file_name}]   Cached:    {cached_checksum}")
                                        LOGGER.debug(f"[v{VERSION}] [{file_name}]   Published: {published_checksum}")
                                    skip_download = False
                            else:
                                LOGGER.warning(f"[v{VERSION}] [{file_name}] Failed to calculate cached file checksum - will download")
                                skip_download = False
                        else:
                            LOGGER.info(f"[v{VERSION}] [{file_name}] No cached file found - will download")
                            skip_download = False
                    else:
                        LOGGER.warning(f"[v{VERSION}] [{file_name}] Invalid checksum format: {published_checksum}")
                        skip_download = False
                else:
                    LOGGER.warning(f"[v{VERSION}] [{file_name}] SHA256 checksum file not available (HTTP {sha256_response.status_code})")
                    skip_download = False
            except Exception as e:
                LOGGER.warning(f"[v{VERSION}] [{file_name}] Failed to check remote checksum: {e}")
                skip_download = False

            # Download file (only if checksum comparison indicated it's needed)
            if not skip_download:
                LOGGER.info(f"[v{VERSION}] [{file_name}] Downloading from {file_url}...")

                # Ensure cache directory exists with explicit error handling
                cache_dir = os.path.dirname(zip_path)
                try:
                    os.makedirs(cache_dir, exist_ok=True)
                    if DEBUG_MODE:
                        LOGGER.debug(f"[v{VERSION}] [{file_name}] Cache directory ready: {cache_dir}")
                except Exception as dir_error:
                    LOGGER.error(f"[v{VERSION}] [{file_name}] Failed to create cache directory '{cache_dir}': {dir_error}")
                    return 2

                # Verify directory exists
                if not os.path.isdir(cache_dir):
                    LOGGER.error(f"[v{VERSION}] [{file_name}] Cache directory does not exist after creation: {cache_dir}")
                    return 2

                for attempt in range(1, max_retries + 1):
                    try:
                        response = get(file_url, timeout=60, stream=True, headers=headers)
                        if response.status_code == 200:
                            with open(zip_path, "wb") as f:
                                for chunk in response.iter_content(chunk_size=8192):
                                    if chunk:
                                        f.write(chunk)
                            file_size = os.path.getsize(zip_path)
                            file_mtime = os.path.getmtime(zip_path)
                            file_mtime_str = datetime.fromtimestamp(file_mtime, tz=timezone.utc).strftime("%a, %d %b %Y %H:%M:%S GMT")
                            LOGGER.info(f"[v{VERSION}] [{file_name}] Download complete: {file_size} bytes")

                            # Verify file integrity with checksum (security enhancement)
                            LOGGER.info(f"[v{VERSION}] [{file_name}] Calculating SHA256 checksum for integrity verification...")
                            is_valid, checksum, actual_size = verify_file_integrity(zip_path, file_size)
                            if not is_valid or not checksum:
                                LOGGER.error(f"[v{VERSION}] [{file_name}] File integrity check failed - file may be corrupted or tampered")
                                # Remove corrupted file
                                try:
                                    os.remove(zip_path)
                                    LOGGER.info(f"[v{VERSION}] [{file_name}] Removed corrupted download")
                                except Exception as e:
                                    LOGGER.warning(f"[v{VERSION}] [{file_name}] Failed to remove corrupted file: {e}")
                                # Continue to retry
                                continue

                            LOGGER.info(f"[v{VERSION}] [{file_name}] ✅ Integrity verified - SHA256: {checksum[:16]}...{checksum[-16:]}")

                            # Save HTTP metadata for future comparisons
                            last_modified_header = response.headers.get('Last-Modified')
                            etag_header = response.headers.get('ETag')
                            content_length_header = response.headers.get('Content-Length')
                            save_zip_metadata(
                                zip_path,
                                last_modified=last_modified_header,
                                etag=etag_header,
                                content_length=content_length_header,
                                sha256=checksum
                            )

                            # Verify against published SHA256 checksum file (additional security layer)
                            # Reuse published_checksum if we already fetched it during pre-download check
                            if not published_checksum:
                                LOGGER.info(f"[v{VERSION}] [{file_name}] Downloading published SHA256 checksum for verification...")
                                sha256_url = file_url + ".sha256"
                                try:
                                    sha256_response = get(sha256_url, timeout=30, headers=headers)
                                    if sha256_response.status_code == 200:
                                        # Parse checksum from .sha256 file
                                        # Format is typically: "checksum  filename" or just "checksum"
                                        published_checksum_line = sha256_response.text.strip()
                                        published_checksum = published_checksum_line.split()[0].strip().lower()

                                        if DEBUG_MODE:
                                            LOGGER.debug(f"[v{VERSION}] [{file_name}] Published checksum file content: {published_checksum_line[:100]}")
                                            LOGGER.debug(f"[v{VERSION}] [{file_name}] Extracted published checksum: {published_checksum}")
                                    else:
                                        LOGGER.warning(f"[v{VERSION}] [{file_name}] SHA256 checksum file not available (HTTP {sha256_response.status_code})")
                                except Exception as e:
                                    LOGGER.warning(f"[v{VERSION}] [{file_name}] Failed to download/verify published checksum: {e}")
                            else:
                                if DEBUG_MODE:
                                    LOGGER.debug(f"[v{VERSION}] [{file_name}] Using published checksum from pre-download check")

                            # Compare checksums if published checksum is available
                            if published_checksum:
                                # Validate checksum format (should be 64 hex chars)
                                if len(published_checksum) == 64 and all(c in '0123456789abcdef' for c in published_checksum):
                                    # Compare checksums (case-insensitive)
                                    if checksum.lower() == published_checksum:
                                        LOGGER.info(f"[v{VERSION}] [{file_name}] ✅ Published checksum verified - matches calculated checksum")
                                    else:
                                        LOGGER.error(f"[v{VERSION}] [{file_name}] ❌ CHECKSUM MISMATCH - File may be tampered or corrupted!")
                                        LOGGER.error(f"[v{VERSION}] [{file_name}]   Published:  {published_checksum}")
                                        LOGGER.error(f"[v{VERSION}] [{file_name}]   Calculated: {checksum.lower()}")
                                        # Remove potentially tampered file
                                        try:
                                            os.remove(zip_path)
                                            LOGGER.info(f"[v{VERSION}] [{file_name}] Removed file with checksum mismatch")
                                        except Exception as e:
                                            LOGGER.warning(f"[v{VERSION}] [{file_name}] Failed to remove file: {e}")
                                        # Continue to retry
                                        continue
                                else:
                                    LOGGER.warning(f"[v{VERSION}] [{file_name}] Invalid checksum format in .sha256 file: {published_checksum}")
                                    LOGGER.warning(f"[v{VERSION}] [{file_name}] Proceeding with calculated checksum only")
                            else:
                                LOGGER.warning(f"[v{VERSION}] [{file_name}] Proceeding with calculated checksum only")

                            remote_modified = file_mtime_str
                            download_checksum = checksum  # Store for state persistence
                            download_ok = True
                            break
                        else:
                            LOGGER.warning(f"[v{VERSION}] [{file_name}] HTTP {response.status_code}")
                    except (ConnectionError, Timeout) as e:
                        LOGGER.warning(f"[v{VERSION}] [{file_name}] Network error: {e}")
                        if attempt < max_retries:
                            delay = 3 * attempt
                            LOGGER.info(f"[v{VERSION}] [{file_name}] Retrying in {delay}s (attempt {attempt}/{max_retries})...")
                            time.sleep(delay)
                    except Exception as e:
                        LOGGER.error(f"[v{VERSION}] [{file_name}] Error downloading: {e}")

                if not download_ok:
                    LOGGER.error(f"[v{VERSION}] [{file_name}] Download failed")
                    return 2

            # At this point, either download succeeded or we're using cached file (skip_download=True)
            # Extract and import
            # Create extraction subdirectory within persistent temp dir (cleaned up after import)
            temp_dir = os.path.join(persistent_temp_dir, f"extract_{file_name}")
            os.makedirs(temp_dir, exist_ok=True)
            if DEBUG_MODE:
                LOGGER.debug(f"[v{VERSION}] [{file_name}] Created extraction directory: {temp_dir}")
                LOGGER.debug(f"[v{VERSION}] [{file_name}] Extracting from ZIP: {zip_path}")
                LOGGER.debug(f"[v{VERSION}] [{file_name}] ZIP size: {os.path.getsize(zip_path)} bytes")
            try:
                with zipfile.ZipFile(zip_path, "r") as zf:
                    zf.extractall(temp_dir)

                # Find all 16 split hash files
                split_files = {}
                bucket_prefixes = ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "A", "B", "C", "D", "E", "F"]
                is_recent = file_name == "recent"

                for root, dirs, files in os.walk(temp_dir):
                    for fname in files:
                        if ("malwarebazaar_full_split_" in fname or "malwarebazaar_recent_split_" in fname) and fname.endswith(".txt"):
                            try:
                                bucket_id = fname.split("_")[-1].replace(".txt", "").upper()
                                if bucket_id in bucket_prefixes:
                                    split_files[bucket_id] = os.path.join(root, fname)
                                    if DEBUG_MODE:
                                        LOGGER.debug(f"[v{VERSION}] [{file_name}] Found split file for bucket {bucket_id}: {fname}")
                            except (IndexError, ValueError):
                                pass

                if not split_files or len(split_files) < 16:
                    LOGGER.error(f"[v{VERSION}] [{file_name}] Expected 16 split files, found {len(split_files)}")
                    return 2

                # Count total lines across all 16 split files before starting import
                total_lines_all_files = 0
                LOGGER.info(f"[v{VERSION}] [{file_name}] Counting total lines in all split files...")
                for bucket_id, file_path in split_files.items():
                    try:
                        with open(file_path, "r") as f:
                            next(f, None)  # Skip header
                            lines = sum(1 for line in f if line.strip() and not line.strip().startswith("#"))
                            total_lines_all_files += lines
                            if DEBUG_MODE:
                                LOGGER.debug(f"[v{VERSION}] [{file_name}] Bucket {bucket_id}: {lines:,} lines")
                    except Exception as e:
                        LOGGER.warning(f"[v{VERSION}] [{file_name}] Failed to count lines for bucket {bucket_id}: {e}")

                LOGGER.info(f"[v{VERSION}] [{file_name}] Total lines to import: {total_lines_all_files:,}")

                # Initialize progress tracking in Redis for UI
                # Set total and reset all per-bucket progress counters to 0
                try:
                    redis_conn.set(f"malware-scan:hashes:import_total:{file_name}", str(total_lines_all_files))
                    # Initialize all per-bucket progress counters to 0
                    for bucket in bucket_prefixes:
                        redis_conn.set(f"malware-scan:hashes:import_processed:{file_name}:bucket_{bucket}", "0")
                    if DEBUG_MODE:
                        LOGGER.debug(f"[v{VERSION}] [{file_name}] Initialized progress tracking: total={total_lines_all_files:,}, per-bucket=0")
                except Exception as e:
                    LOGGER.warning(f"[v{VERSION}] [{file_name}] Failed to initialize progress tracking in Redis: {e}")

                # Import split files in parallel
                bucket_prefix = "recent" if is_recent else ""
                bucket_prefix_display = " (recent)" if is_recent else " (full)"
                if DEBUG_MODE:
                    LOGGER.debug(f"[v{VERSION}] [{file_name}] Importing to {bucket_prefix if bucket_prefix else 'full'} hash table{bucket_prefix_display}")

                added_total = 0
                duplicates_total = 0
                processed_total_all_buckets = 0
                import_lock = threading.Lock()
                import_results = {}

                def import_bucket_file(bucket_id, file_path, bucket_prefix=""):
                    """Import a single bucket's split file to appropriate hash table"""
                    try:
                        added = 0
                        duplicates = 0
                        bucket_key_suffix = f"recent:{bucket_id}" if bucket_prefix else bucket_id
                        bucket = f"malware_scan:hashes:{bucket_key_suffix}"

                        # Get existing count before import
                        existing_before = redis_conn.hlen(bucket) or 0

                        # Count total lines in file (excluding header)
                        total_lines = 0
                        try:
                            with open(file_path, "r") as f:
                                next(f, None)  # Skip header
                                total_lines = sum(1 for line in f if line.strip() and not line.strip().startswith("#"))
                        except Exception as e:
                            LOGGER.warning(f"[v{VERSION}] [{file_name}] Failed to count lines for bucket {bucket_id}: {e}")
                            total_lines = 0  # Fallback to unknown count

                        start_time = time.time()
                        last_progress_time = start_time

                        with open(file_path, "r") as f:
                            next(f, None)  # Skip header
                            for line in f:
                                line = line.strip()
                                if not line or line.startswith("#"):
                                    continue

                                fields = line.split(",")
                                if len(fields) < 3:
                                    continue

                                sha256 = fields[0].strip().upper()
                                signature = fields[1].strip()
                                clamav = fields[2].strip()

                                if not sha256 or len(sha256) != 64:
                                    continue

                                # Store as "signature|clamav" (pipe-delimited)
                                value = f"{signature}|{clamav}"

                                if redis_conn.hexists(bucket, sha256):
                                    duplicates += 1
                                else:
                                    redis_conn.hset(bucket, sha256, value)
                                    added += 1

                                # Progress update every 100k hashes or every 10 seconds
                                current_time = time.time()
                                # Log progress every 10 seconds (regardless of debug mode)
                                if (current_time - last_progress_time) >= 10:
                                    elapsed = current_time - start_time
                                    rate = (added + duplicates) / elapsed if elapsed > 0 else 0
                                    total_after = existing_before + added
                                    processed = added + duplicates

                                    # Format progress message with "nnn of xxx" if total is known
                                    if total_lines > 0:
                                        progress_str = f"{processed:,} of {total_lines:,}"
                                        percent = (processed / total_lines * 100) if total_lines > 0 else 0
                                        LOGGER.info(f"[v{VERSION}] [{file_name}] Bucket {bucket_id} progress: {progress_str} ({percent:.1f}%), added={added:,}, duplicates={duplicates:,}, rate={rate:.0f}h/s")
                                    else:
                                        LOGGER.info(f"[v{VERSION}] [{file_name}] Bucket {bucket_id} progress: processed={processed:,}, added={added:,}, duplicates={duplicates:,}, rate={rate:.0f}h/s")

                                    # Update global progress counter in Redis for UI tracking
                                    try:
                                        # Use SET to track per-bucket progress (will be summed by API)
                                        redis_conn.set(f"malware-scan:hashes:import_processed:{file_name}:bucket_{bucket_id}", str(processed))
                                    except Exception as e:
                                        if DEBUG_MODE:
                                            LOGGER.debug(f"[v{VERSION}] [{file_name}] Failed to update progress counter: {e}")

                                    # Renew import lock TTL to prevent expiration during long imports
                                    try:
                                        redis_conn.expire(lock_key, 60)
                                    except Exception:
                                        pass  # Lock renewal failure is non-critical
                                    last_progress_time = current_time

                        # Set TTL for recent buckets (48 hours)
                        if bucket_prefix == "recent":
                            ttl_seconds = 48 * 60 * 60
                            redis_conn.expire(bucket, ttl_seconds)
                            if DEBUG_MODE:
                                LOGGER.debug(f"[v{VERSION}] [{file_name}] Bucket {bucket_id} TTL set to 48 hours")

                        # Get final count after import
                        total_after = redis_conn.hlen(bucket) or 0

                        with import_lock:
                            import_results[bucket_id] = (added, duplicates)

                        # Update final progress for this bucket
                        try:
                            final_processed = added + duplicates
                            redis_conn.set(f"malware-scan:hashes:import_processed:{file_name}:bucket_{bucket_id}", str(final_processed))
                        except Exception as e:
                            if DEBUG_MODE:
                                LOGGER.debug(f"[v{VERSION}] [{file_name}] Failed to update final progress counter: {e}")

                        # Log completion with total lines info
                        if DEBUG_MODE:
                            processed_total = added + duplicates
                            if total_lines > 0:
                                LOGGER.debug(f"[v{VERSION}] [{file_name}] Bucket {bucket_id} import complete: processed {processed_total} of {total_lines} ({processed_total/total_lines*100:.1f}%), before={existing_before}, added={added}, duplicates={duplicates}, after={total_after}")
                            else:
                                LOGGER.debug(f"[v{VERSION}] [{file_name}] Bucket {bucket_id} import complete: before={existing_before}, added={added}, duplicates={duplicates}, after={total_after}")
                        return True
                    except Exception as e:
                        LOGGER.error(f"[v{VERSION}] [{file_name}] Import error for bucket {bucket_id}: {e}")
                        return False

                # Import all 16 files in parallel
                LOGGER.info(f"[v{VERSION}] [{file_name}] Starting parallel import of 16 buckets{bucket_prefix_display}...")
                with ThreadPoolExecutor(max_workers=16) as executor:
                    futures = {
                        executor.submit(import_bucket_file, bucket_id, file_path, bucket_prefix): bucket_id
                        for bucket_id, file_path in split_files.items()
                    }

                    failed_buckets = []
                    for future in as_completed(futures):
                        bucket_id = futures[future]
                        try:
                            success = future.result()
                            if not success:
                                failed_buckets.append(bucket_id)
                        except Exception as e:
                            LOGGER.error(f"[v{VERSION}] [{file_name}] Thread error for bucket {bucket_id}: {e}")
                            failed_buckets.append(bucket_id)

                    if failed_buckets:
                        LOGGER.warning(f"[v{VERSION}] [{file_name}] Failed to import buckets: {', '.join(failed_buckets)}")
                        return 2

                    # Sum results from all buckets
                    for added, duplicates in import_results.values():
                        added_total += added
                        duplicates_total += duplicates

                    LOGGER.info(f"[v{VERSION}] [{file_name}] Import complete: new_entries={added_total:,}, already_existed={duplicates_total:,}, processed={added_total + duplicates_total:,}")

                    # Count totals per bucket and show distribution
                    table_name = "recent" if is_recent else "full"
                    total = 0
                    min_count = None
                    max_count = None
                    for prefix in bucket_prefixes:
                        bucket_key_suffix = f"recent:{prefix}" if is_recent else prefix
                        count = redis_conn.hlen(f"malware_scan:hashes:{bucket_key_suffix}") or 0
                        total += count
                        if min_count is None or count < min_count:
                            min_count = count
                        if max_count is None or count > max_count:
                            max_count = count
                        if DEBUG_MODE:
                            LOGGER.debug(f"[v{VERSION}] [{file_name}] Bucket {prefix}: {count:,} hashes")

                    avg_count = total // 16 if total > 0 else 0
                    balance_ratio = (max_count / min_count) if min_count and min_count > 0 else 1.0
                    LOGGER.info(f"[v{VERSION}] [{file_name}] Total hashes in {table_name} table: {total:,} across 16 buckets (avg={avg_count:,}, min={min_count:,}, max={max_count:,}, ratio={balance_ratio:.3f})")

                    # Record import end time for this file type
                    import time as time_module
                    import_end_time_key = f"malware-scan:hashes:import_end_time:{file_name}"
                    try:
                        redis_conn.set(import_end_time_key, str(int(time_module.time())))
                    except Exception as e:
                        LOGGER.warning(f"[v{VERSION}] [{file_name}] Failed to set import end time: {e}")

                    # Update state in BunkerWeb cache with success marker
                    state_content = (
                        f'SAVED_ETAG="{remote_etag}"\n'
                        f'SAVED_SIZE="{remote_size}"\n'
                        f'SAVED_MODIFIED="{remote_modified}"\n'
                        f'SAVED_SHA256="{download_checksum}"\n'
                        f'DOWNLOAD_TIME="{datetime.now(timezone.utc).isoformat()}"\n'
                        f'IMPORT_SUCCESS="yes"\n'
                    )
                    cached, err = JOB.cache_file(state_key, state_content.encode())
                    if not cached:
                        LOGGER.warning(f"[v{VERSION}] [{file_name}] Failed to cache state: {err}")

                    # Keep downloaded ZIP file for potential future use
                    LOGGER.info(f"[v{VERSION}] [{file_name}] Downloaded ZIP file preserved at: {zip_path}")

                    file_status = 1 if added_total > 0 else 0

            finally:
                shutil.rmtree(temp_dir, ignore_errors=True)

            return file_status

        except Exception as e:
            LOGGER.error(f"[v{VERSION}] [{file_name}] Error: {e}")
            return 2

    # Process both full and recent files
    LOGGER.info(f"[v{VERSION}] Starting malware hash database updates...")

    # Import state key for tracking interrupted imports
    import_state_key = "malware-scan:hashes:import_state"

    # Check for interrupted import from previous run AND save previous state
    force_reimport = False
    previous_import_state = None
    try:
        import_state = redis_conn.get(import_state_key)
        if import_state:
            previous_import_state = import_state.decode()
            if previous_import_state == "running":
                # Previous import didn't complete - check if lock is still held
                lock_exists = redis_conn.exists(lock_key)
                if not lock_exists:
                    LOGGER.warning(f"[v{VERSION}] Detected interrupted import from previous run - forcing full reimport")
                    force_reimport = True
                    # Delete state keys to trigger full cleanup and reimport
                    try:
                        redis_conn.delete(state_key_full)
                        redis_conn.delete(state_key_recent)
                        LOGGER.info(f"[v{VERSION}] Deleted state keys to trigger full reimport")
                    except Exception as e:
                        LOGGER.warning(f"[v{VERSION}] Failed to delete state keys: {e}")
    except Exception as e:
        LOGGER.warning(f"[v{VERSION}] Failed to check import state: {e}")

    # Acquire lock to prevent concurrent imports
    lock_acquired = False
    lock_refresh_thread = None
    lock_refresh_stop = threading.Event()

    def refresh_lock():
        """Background thread function that refreshes the import lock TTL every 2 seconds."""
        while not lock_refresh_stop.is_set():
            try:
                # Sleep for 2 seconds before refreshing
                if lock_refresh_stop.wait(timeout=2.0):
                    break  # Stop event was set, exit loop

                # Refresh lock TTL to 10 seconds using main connection
                redis_conn.expire(lock_key, 10)
                if DEBUG_MODE:
                    LOGGER.debug(f"[v{VERSION}] Lock TTL refreshed to 10s")
            except Exception as e:
                LOGGER.warning(f"[v{VERSION}] Lock refresh error: {e}")

    try:
        # Try to acquire lock with 10-second TTL
        lock_acquired = redis_conn.set(lock_key, "1", nx=True, ex=10)
        if not lock_acquired:
            LOGGER.warning(f"[v{VERSION}] Another import is in progress, waiting 20 seconds for lock...")
            # Wait 20 seconds and try one more time
            time.sleep(20)
            lock_acquired = redis_conn.set(lock_key, "1", nx=True, ex=10)
            if not lock_acquired:
                LOGGER.warning(f"[v{VERSION}] Could not acquire import lock after 20s wait, skipping this run")
                sys_exit(0)

        if DEBUG_MODE:
            LOGGER.debug(f"[v{VERSION}] Import lock acquired with 10s TTL")

        # Start background thread to refresh lock every 2 seconds
        lock_refresh_thread = threading.Thread(target=refresh_lock, daemon=True, name="LockRefresh")
        lock_refresh_thread.start()
        LOGGER.info(f"[v{VERSION}] Lock refresh heartbeat started (10s TTL, 2s refresh interval)")

        # Set import state to "running"
        try:
            redis_conn.set(import_state_key, "running")
            LOGGER.info(f"[v{VERSION}] Import state set to 'running'")
        except Exception as e:
            LOGGER.warning(f"[v{VERSION}] Failed to set import state: {e}")

        # If forced reimport, delete all hash buckets to start fresh
        if force_reimport:
            LOGGER.info(f"[v{VERSION}] [RECOVERY] Cleaning existing hash buckets for full reimport...")
            mb_buckets_deleted = 0
            bucket_prefixes = ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "A", "B", "C", "D", "E", "F"]

            # Delete full hash buckets
            for prefix in bucket_prefixes:
                bucket = f"malware_scan:hashes:{prefix}"
                try:
                    if redis_conn.delete(bucket):
                        mb_buckets_deleted += 1
                except Exception as e:
                    LOGGER.warning(f"[v{VERSION}] [RECOVERY] Failed to delete bucket {bucket}: {e}")

            # Delete recent hash buckets
            for prefix in bucket_prefixes:
                bucket = f"malware_scan:hashes:recent:{prefix}"
                try:
                    if redis_conn.delete(bucket):
                        mb_buckets_deleted += 1
                except Exception as e:
                    LOGGER.warning(f"[v{VERSION}] [RECOVERY] Failed to delete bucket {bucket}: {e}")

            if mb_buckets_deleted > 0:
                LOGGER.info(f"[v{VERSION}] [RECOVERY] Deleted {mb_buckets_deleted} hash buckets for clean reimport")

        for file_config in files_to_update:
            file_status = process_file(file_config, previous_import_state)
            if file_status == 2:
                status = 2  # Error occurred
            elif file_status == 1 and status != 2:
                status = 1  # Changes made (if no error)

        LOGGER.info(f"[v{VERSION}] Malware hash database updates complete (status={status})")

        # Set import state to "success" if no errors occurred
        if status != 2:
            try:
                redis_conn.set(import_state_key, "success")
                LOGGER.info(f"[v{VERSION}] ✅ Import state set to 'success' (status={status})")
            except Exception as e:
                LOGGER.warning(f"[v{VERSION}] Failed to set import success state: {e}")
        else:
            LOGGER.warning(f"[v{VERSION}] Import completed with errors - state remains 'running' for recovery")

    finally:
        # Stop lock refresh thread if it's running
        if lock_refresh_thread and lock_refresh_thread.is_alive():
            lock_refresh_stop.set()
            lock_refresh_thread.join(timeout=3.0)  # Wait up to 3 seconds for thread to stop
            if DEBUG_MODE:
                LOGGER.debug(f"[v{VERSION}] Lock refresh heartbeat stopped")

        # Release lock if we acquired it
        if lock_acquired:
            try:
                redis_conn.delete(lock_key)
                if DEBUG_MODE:
                    LOGGER.debug(f"[v{VERSION}] Import lock released")
            except Exception as e:
                LOGGER.warning(f"[v{VERSION}] Failed to release lock (will expire in 10s): {e}")

    # Mark state keys in Redis to indicate initialization is complete
    # This prevents cleanup from running on subsequent job executions
    # Always set these keys (not just on first run) to ensure they persist across restarts
    try:
        if DEBUG_MODE:
            LOGGER.debug(f"[v{VERSION}] [STATE] Setting Redis state keys to mark initialization complete")
            LOGGER.debug(f"[v{VERSION}] [STATE]   Key: malware-scan:hashes:state:full")
            LOGGER.debug(f"[v{VERSION}] [STATE]   Key: malware-scan:hashes:state:recent")

        redis_conn.set("malware-scan:hashes:state:full", "1")
        redis_conn.set("malware-scan:hashes:state:recent", "1")
        if DEBUG_MODE:
            LOGGER.debug(f"[v{VERSION}] [STATE] Redis state keys marked for initialization completion")

        if DEBUG_MODE:
            # Verify keys were set
            full_check = redis_conn.exists("malware-scan:hashes:state:full")
            recent_check = redis_conn.exists("malware-scan:hashes:state:recent")
            LOGGER.debug(f"[v{VERSION}] [STATE] Verification:")
            LOGGER.debug(f"[v{VERSION}] [STATE]   Full key exists: {full_check}")
            LOGGER.debug(f"[v{VERSION}] [STATE]   Recent key exists: {recent_check}")
    except Exception as e:
        LOGGER.warning(f"[v{VERSION}] [STATE] Failed to set state keys in Redis: {e}")

except SystemExit as e:
    status = e.code
except BaseException as e:
    status = 2
    LOGGER.debug(format_exc())
    LOGGER.error(f"[v{VERSION}] Exception while running malware-scan-update.py:\n{e}")

# Log completion
if DEBUG_MODE:
    status_name = "SUCCESS" if status == 0 else "CHANGED" if status == 1 else "ERROR"
    LOGGER.debug(f"[v{VERSION}] Script completed: {SCRIPT_NAME}")
    LOGGER.debug(f"[v{VERSION}] Exit status: {status} ({status_name})")

sys_exit(status)
