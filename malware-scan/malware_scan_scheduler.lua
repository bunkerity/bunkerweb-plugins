-- BunkerWeb Malware Scan - Scheduler Module
-- Handles automatic MalwareBazaar custom hash database updates
-- Uses ETag-based caching for smart downloads and Redis for bulk imports
--
-- Features:
-- - Scheduled downloads with intelligent change detection (ETag + filesize)
-- - Configurable update intervals (seconds precision)
-- - State persistence for bandwidth optimization (99.9% savings)
-- - Atomic database updates with bucketed key distribution (16 buckets)
-- - Infinite persistence (no TTL - freshness maintained by scheduler)
-- - Support for three strategies: full, recent, hybrid
--
-- Version: 0.7.44
-- Date: 2026-01-27

local scheduler = {}

-- Module version
scheduler.VERSION = "0.8.0"

local ngx = ngx
local tonumber = tonumber
local tostring = tostring
local lfs = require("lfs")

-- Load centralized logger and utility modules
local logger = require("malware_scan_logger")
local utils = require("malware_scan_utils")
local database = require("malware_scan_database")

-- ============================================================================
-- TYPE DEFINITIONS
-- ============================================================================

--- @class Plugin
--- @field variables table<string, string> Configuration variables from BunkerWeb
--- @field use_redis boolean Whether Redis is enabled
--- @field clusterstore table|nil Clusterstore instance for Redis access
--- @field redisdb table|nil Redis client instance
--- @field datastore table|nil Datastore instance for key-value operations
--- @field cachestore table Local cache store instance
--- @field ctx table Request context with nested tables
--- @field logger table Logger instance with logging methods
--- @field ret fun(self: Plugin, ...): table Plugin return method (variadic)
--- @field log_debug fun(self: Plugin, message: string) Debug logging method
--- @field sanitize_external_data fun(self: Plugin, data: string): string Data sanitization method
--- @field detection_share fun(self: Plugin, ...) Detection sharing method
--- @field mask_sensitive fun(...): string Sensitive data masking method
--- @field track_file_scanned fun(): boolean File tracking method
--- @field file_get_size fun(...): number|nil Get file size method
--- @field hash_calculate fun(...): string|nil Calculate hash method

-- ============================================================================
-- HELPER FUNCTIONS
-- ============================================================================

--- Create directory recursively (like mkdir -p)
--- @param path string Directory path to create
--- @return boolean success True if directory was created/exists
--- @return string|nil error Error message if failed
local function mkdir_recursive(path)
	if not path or path == "" then
		return false, "path is empty"
	end

	-- Check if directory already exists
	local attr = lfs.attributes(path, "mode")
	if attr == "directory" then
		return true, nil
	elseif attr then
		return false, "path exists but is not a directory"
	end

	-- Split path into components
	local components = {}
	for component in path:gmatch("[^/]+") do
		table.insert(components, component)
	end

	-- Build path incrementally
	local current_path = path:sub(1, 1) == "/" and "/" or ""
	for _, component in ipairs(components) do
		current_path = current_path .. component .. "/"

		-- Check if this level exists
		local level_attr = lfs.attributes(current_path, "mode")
		if not level_attr then
			-- Create this directory level
			local ok, err = lfs.mkdir(current_path)
			if not ok then
				return false, "failed to create directory " .. current_path .. ": " .. (err or "unknown error")
			end
		elseif level_attr ~= "directory" then
			return false, "path component exists but is not a directory: " .. current_path
		end
	end

	return true, nil
end

--- Create secure temporary directory using native Lua
--- @return string|nil temp_dir Temporary directory path, or nil on error
local function create_temp_dir()
	-- Use os.tmpname() to get a unique temporary path
	local tmpname = os.tmpname()
	if not tmpname then
		return nil
	end

	-- Extract just the filename part to create a unique suffix
	local suffix = tmpname:match("([^/]+)$") or tostring(os.time())

	-- Build secure temp directory path
	local temp_dir = "/tmp/malware_scheduler." .. suffix

	-- Create the directory
	local ok, err = lfs.mkdir(temp_dir)
	if not ok then
		logger.log_error("[SCHEDULER] Failed to create temp directory: " .. (err or "unknown"))
		return nil
	end

	-- Verify directory was created
	local attr = lfs.attributes(temp_dir, "mode")
	if attr ~= "directory" then
		return nil
	end

	return temp_dir
end

-- ============================================================================
-- REDIS CONFIGURATION VALIDATION
-- ============================================================================

-- Validate Redis configuration and availability.
-- Checks that Redis is properly configured when the plugin is enabled.
--
--- @param plugin Plugin instance
--- @return boolean valid True if Redis is available and configured
--- @return string|nil error_message Error message if validation fails
local function scheduler_validate_redis_config(plugin)
	-- Check if BunkerWeb's global USE_REDIS is enabled
	local use_redis = plugin.variables["USE_REDIS"] == "yes"

	if not use_redis then
		return false, "Redis is not enabled in BunkerWeb configuration (USE_REDIS != yes). " ..
		              "Custom hash database requires Redis. " ..
		              "Please set USE_REDIS=yes and configure either REDIS_HOST or REDIS_SENTINEL_HOSTS."
	end

	-- Check if Redis connection is available
	if not database.is_available(plugin) then
		return false, "Redis connection is not available. " ..
		              "USE_REDIS is enabled but Redis connection failed. " ..
		              "Please verify your Redis configuration: " ..
		              "either REDIS_HOST (for standalone Redis) or REDIS_SENTINEL_HOSTS (for Sentinel) must be properly configured."
	end

	-- Redis is properly configured and connected
	return true, nil
end

-- ============================================================================
-- STATE FILE MANAGEMENT
-- ============================================================================

-- Load state file from disk
--- @param state_file string Path to state file
--- @return table state State table with saved_etag, saved_size, saved_modified, etc.
local function scheduler_load_state(state_file)
	local state = {
		saved_etag = nil,
		saved_size = nil,
		saved_modified = nil,
		last_update = nil,
		full_update_delay_seconds = nil,
		full_update_delay_date = nil,
		last_full_update = nil
	}

	local f, err = io.open(state_file, "r")
	if not f then
		return state  -- No state file yet
	end

	for line in f:lines() do
		local key, value = line:match("^(%w+)=(.*)$")
		if key and value then
			value = value:gsub('^"', ''):gsub('"$', '')  -- Remove quotes
			state[key] = value
		end
	end
	f:close()

	return state
end

-- Save state file to disk
-- Can be called with individual parameters: scheduler_save_state(path, etag, size, modified)
-- Or with state table: scheduler_save_state(path, nil, nil, nil) and pass table as state_obj
--- @param state_file string Path to state file
--- @param etag string|table|nil Remote ETag value (or table with state if others are nil)
--- @param size number|nil Remote file size
--- @param modified string|nil Remote Last-Modified header
--- @param state_obj table|nil Optional - full state table to merge
local function scheduler_save_state(state_file, etag, size, modified, state_obj)
	local state_dir = state_file:match("(.*/)")
	if state_dir and state_dir ~= "" then
		-- Create state directory using native Lua (non-blocking)
		local mkdir_ok, mkdir_err = mkdir_recursive(state_dir)
		if not mkdir_ok then
			logger.log_error("[SCHEDULER] Failed to create state directory: " .. (mkdir_err or "unknown error"))
		end
	end

	-- If etag is a table, treat it as state object
	if type(etag) == "table" then
		state_obj = etag
		etag = state_obj.saved_etag
		size = state_obj.saved_size
		modified = state_obj.saved_modified
	end

	local f, err = io.open(state_file, "w")
	if not f then
		logger.log_error( "[SCHEDULER] Failed to save state: " .. (err or "unknown error"))
		return false
	end

	f:write('SAVED_ETAG="' .. (etag or "") .. '"\n')
	f:write('SAVED_SIZE="' .. (size or "") .. '"\n')
	f:write('SAVED_MODIFIED="' .. (modified or "") .. '"\n')
	f:write('DOWNLOAD_TIME="' .. os.date("!%Y-%m-%dT%H:%M:%SZ") .. '"\n')

	-- Write full update delay info (for server load balancing)
	if state_obj then
		f:write('FULL_UPDATE_DELAY_SECONDS="' .. (state_obj.full_update_delay_seconds or "") .. '"\n')
		f:write('FULL_UPDATE_DELAY_DATE="' .. (state_obj.full_update_delay_date or "") .. '"\n')
		f:write('LAST_FULL_UPDATE="' .. (state_obj.last_full_update or "") .. '"\n')
	end

	f:close()
	return true
end

-- ============================================================================
-- REMOTE METADATA RETRIEVAL
-- ============================================================================

-- Get remote file metadata via HTTP HEAD request
--- @param url string Download URL
--- @return string|nil etag Remote ETag header, or nil if failed
--- @return string|nil size Remote Content-Length header, or nil if failed
--- @return string|nil modified Remote Last-Modified header, or nil if failed
local function scheduler_get_remote_metadata(url)
	local httpc = require("resty.http").new()
	if not httpc then
		logger.log_error( "[SCHEDULER] Failed to create HTTP client")
		return nil, nil, nil
	end

	httpc:set_timeout(10000)

	local res, err = httpc:request_uri(url, {
		method = "HEAD",
		ssl_verify = true
	})

	if not res then
		logger.log_error( "[SCHEDULER] HTTP HEAD failed: " .. (err or "unknown"))
		return nil, nil, nil
	end

	if res.status ~= 200 and res.status ~= 304 then
		logger.log_error( "[SCHEDULER] HTTP " .. res.status .. " error")
		return nil, nil, nil
	end

	local headers = res.headers or {}
	local etag = headers["etag"] or headers["ETag"] or ""
	local size = headers["content-length"] or headers["Content-Length"] or ""
	local modified = headers["last-modified"] or headers["Last-Modified"] or ""

	return etag, size, modified
end

-- ============================================================================
-- FILE DOWNLOAD
-- ============================================================================

-- Download file from URL
--- @param url string Download URL
--- @param output_path string Local file path to save to
--- @return boolean success True on success, false on failure
local function scheduler_download_file(url, output_path)
	local output_dir = output_path:match("(.*/)")
	if output_dir and output_dir ~= "" then
		-- Create output directory using native Lua (non-blocking)
		local mkdir_ok, mkdir_err = mkdir_recursive(output_dir)
		if not mkdir_ok then
			logger.log_error("[SCHEDULER] Failed to create output directory: " .. (mkdir_err or "unknown error"))
		end
	end

	local httpc = require("resty.http").new()
	if not httpc then
		logger.log_error( "[SCHEDULER] Failed to create HTTP client")
		return false
	end

	httpc:set_timeout(60000)  -- 60 second timeout for large files

	local res, err = httpc:request_uri(url, {
		method = "GET",
		ssl_verify = true
	})

	if not res then
		logger.log_error( "[SCHEDULER] Download failed: " .. (err or "unknown"))
		return false
	end

	if res.status ~= 200 then
		logger.log_error( "[SCHEDULER] HTTP " .. res.status .. " error")
		return false
	end

	local f, ferr = io.open(output_path, "wb")
	if not f then
		logger.log_error( "[SCHEDULER] Failed to write file: " .. (ferr or "unknown"))
		return false
	end

	if res.body then
		f:write(res.body)
	end
	f:close()

	return true
end

-- ============================================================================
-- CSV EXTRACTION AND IMPORT
-- ============================================================================

-- Create secure temporary directory using native Lua (non-blocking).
-- Returns a random directory path that is unpredictable.
--- @return string|nil temp_dir Temporary directory path, or nil on error
local function scheduler_create_temp_dir()
	-- Use the native helper function defined at the top of the file
	return create_temp_dir()
end

-- Store temp directory in Redis for cleanup tracking.
-- Allows recovery of abandoned temp directories from interrupted runs.
--- @param plugin Plugin Plugin instance
--- @param temp_dir string Temporary directory path
local function scheduler_register_temp_dir(plugin, temp_dir)
	if not plugin.redisdb then
		return false
	end

	-- Store with 24-hour TTL (cleanup will happen much sooner, but this prevents leaks)
	local key = "malware_scan:scheduler:temp_dir"
	database.setex(plugin, key, 86400, temp_dir)
	return true
end

-- Cleanup temp directory and unregister from Redis.
--- @param plugin Plugin Plugin instance
--- @param temp_dir string Temporary directory path
local function scheduler_cleanup_temp_dir(plugin, temp_dir)
	if temp_dir and temp_dir ~= "" then
		os.execute("rm -rf " .. utils.shell_escape_path(temp_dir))
	end

	if plugin.redisdb then
		database.unlink(plugin, "malware_scan:scheduler:temp_dir")
	end
end

-- Cleanup any abandoned temp directories from previous interrupted runs.
-- Checks Redis for registered temp directories and removes them.
--- @param plugin Plugin instance
local function scheduler_cleanup_abandoned_temp_dirs(plugin)
	if not plugin.redisdb then
		return
	end

	local key = "malware_scan:scheduler:temp_dir"
	local old_temp_dir = database.redis_get(plugin, key)

	if old_temp_dir and old_temp_dir ~= ngx.null then
		logger.log_notice( "[SCHEDULER] Cleaning up abandoned temp directory: " .. old_temp_dir)
		os.execute("rm -rf " .. utils.shell_escape_path(old_temp_dir))
		database.unlink(plugin, key)
	end
end

-- Extract ZIP and import CSV to Redis
--- @param zip_file string Path to downloaded ZIP file
--- @param plugin Plugin Plugin instance (for Redis access)
--- @return number added Count of hashes added to database
--- @return number duplicates Count of duplicate hashes skipped
local function scheduler_import_from_zip(zip_file, plugin)
	if not plugin.redisdb then
		logger.log_error( "[SCHEDULER] Redis not available")
		return 0, 0
	end

	-- Clean up any abandoned temp directories from previous runs
	scheduler_cleanup_abandoned_temp_dirs(plugin)

	-- Extract ZIP in secure temp location using mktemp
	local work_dir = scheduler_create_temp_dir()
	if not work_dir then
		logger.log_error( "[SCHEDULER] Failed to create temporary directory")
		return 0, 0
	end

	-- Register temp directory for cleanup tracking
	scheduler_register_temp_dir(plugin, work_dir)

	os.execute("cd " .. utils.shell_escape_path(work_dir) .. " && unzip -q " .. utils.shell_escape_path(zip_file))

	-- Find CSV file using native Lua (non-blocking)
	local csv_file = nil
	for filename in lfs.dir(work_dir) do
		if filename:match("%.csv$") then
			local file_path = work_dir .. "/" .. filename
			local attr = lfs.attributes(file_path, "mode")
			if attr == "file" then
				csv_file = file_path
				break  -- Use first CSV file found
			end
		end
	end

	if not csv_file then
		logger.log_error( "[SCHEDULER] No CSV file found in ZIP")
		scheduler_cleanup_temp_dir(plugin, work_dir)
		return 0, 0
	end

	local added = 0
	local duplicates = 0
	local hash_key = "malware_scan:hashes"

	-- Import CSV lines
	local f, err = io.open(csv_file, "r")
	if not f then
		logger.log_error( "[SCHEDULER] Failed to read CSV: " .. (err or "unknown"))
		scheduler_cleanup_temp_dir(plugin, work_dir)
		return 0, 0
	end

	for line in f:lines() do
		-- Skip comment lines
		if not line:match("^#") and line ~= "" then
			-- Simple CSV parsing (quoted fields)
			local fields = {}
			for field in line:gmatch('"([^"]*)"') do
				table.insert(fields, field)
			end

			if #fields >= 9 then
				local sha256 = fields[2]
				local file_type = fields[7]
				local mime_type = fields[8]
				local signature = fields[9]

				-- Validate SHA256
				if sha256 and #sha256 == 64 and sha256:match("^[a-fA-F0-9]+$") then
					-- Skip if no signature
					if signature and signature ~= "n/a" and signature ~= "" then
						-- Check if exists
						if not database.hexists(plugin, hash_key, sha256) then
							local metadata = file_type .. "|" .. mime_type
							local data = signature .. "|" .. metadata
							database.hset(plugin, hash_key, sha256, data)
							added = added + 1
						else
							duplicates = duplicates + 1
						end
					end
				end
			end
		end
	end
	f:close()

	-- No TTL set on hashes - they persist indefinitely
	-- Freshness maintained by:
	-- - Daily full refresh (at 3 AM with hybrid strategy)
	-- - Frequent incremental updates (every 10 minutes)
	-- - ETag-based change detection (avoids unnecessary re-imports)
	logger.log_notice( "[SCHEDULER] Import complete - hashes persist indefinitely (no TTL)")

	scheduler_cleanup_temp_dir(plugin, work_dir)
	return added, duplicates
end

-- ============================================================================
-- UPDATE STRATEGY SUPPORT
-- ============================================================================

-- Determine which URL to use based on strategy
--- @param base_url string Base URL from configuration
--- @param strategy string Update strategy (full, recent, hybrid)
--- @return string url URL to download from
local function scheduler_get_download_url(base_url, strategy)
	if not strategy or strategy == "full" then
		-- Use full database
		return base_url or "https://bazaar.sysangels.ai/bunkerweb/malwarebazaar_full_hash.txt.zip"
	elseif strategy == "recent" or strategy == "hybrid" then
		-- Use recent/incremental updates
		return (base_url:gsub("full_hash", "recent_hash")) or
		       "https://bazaar.sysangels.ai/bunkerweb/malwarebazaar_recent_hash.txt.zip"
	end
	return base_url
end

-- Generate random delay for full update (1-9 minutes) to prevent server overload
--- @return number delay Random delay in seconds (60-540)
local function scheduler_get_random_full_update_delay()
	math.randomseed(os.time())
	-- Random 1-9 minutes = 60-540 seconds
	return math.random(1, 9) * 60
end

-- Calculate scheduled time for full update (3 AM UTC + random delay)
--- @return number scheduled_time Seconds since midnight for scheduled full update
local function scheduler_get_full_update_scheduled_time(delay_seconds)
	-- 3 AM = 3 * 3600 = 10800 seconds
	-- Add random delay (1-9 minutes)
	return 10800 + delay_seconds
end

-- Get current time as seconds since midnight UTC
--- @return number seconds Seconds since midnight UTC
local function scheduler_get_current_time_of_day()
	return tonumber(os.date("!%H")) * 3600 + tonumber(os.date("!%M")) * 60 + tonumber(os.date("!%S"))
end

-- Check if we should do full update today (for hybrid strategy)
-- Incorporates random 1-9 minute delay to prevent server overload at exactly 3 AM
--- @param state_file string Path to state file
--- @return boolean should_update True if should do full update, false for recent
--- @return number delay_seconds Random delay in seconds (informational, for logging)
local function scheduler_should_do_full_today(state_file)
	local state = scheduler_load_state(state_file)
	local today = os.date("!%Y-%m-%d")

	-- Check if we've already done a full update today
	if state.last_full_update then
		local last_date = state.last_full_update:match("^(%d%d%d%d%-%d%d%-%d%d)")
		if last_date == today then
			-- Already did full update today
			return false, 0
		end
	end

	-- New day - check if we should do full update now
	-- Generate delay if not already set for today
	local delay_seconds = tonumber(state.full_update_delay_seconds) or 0

	if delay_seconds == 0 or state.full_update_delay_date ~= today then
		-- First check of the day - generate random delay
		delay_seconds = scheduler_get_random_full_update_delay()
		-- Save delay for rest of the day
		state.full_update_delay_seconds = tostring(delay_seconds)
		state.full_update_delay_date = today
		scheduler_save_state(state_file, state.saved_etag, state.saved_size, state.saved_modified)
	end

	-- Check if current time has passed 3 AM + delay
	local scheduled_time = scheduler_get_full_update_scheduled_time(delay_seconds)
	local current_time = scheduler_get_current_time_of_day()

	if current_time >= scheduled_time then
		-- Time to do full update
		logger.log_notice( string.format("[v" .. scheduler.VERSION .. "] [SCHEDULER] Full update scheduled at 3:00 AM + %d minutes, now executing", delay_seconds / 60))
		return true, delay_seconds
	else
		-- Still waiting for scheduled time
		local time_until = scheduled_time - current_time
		logger.log_notice( string.format("[v" .. scheduler.VERSION .. "] [SCHEDULER] Hybrid: waiting for full update (scheduled in %d seconds)", time_until))
		return false, delay_seconds
	end
end

-- ============================================================================
-- STARTUP HASH INITIALIZATION
-- ============================================================================

-- Check if hash buckets exist in Redis
--- @param plugin Plugin instance
--- @return boolean exists True if any buckets have data, false if all empty
local function scheduler_hashes_exist(plugin)
	if not plugin.redisdb then
		return false
	end

	-- Check if any of the 16 buckets have data
	local bucket_prefixes = {"0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "A", "B", "C", "D", "E", "F"}

	for _, prefix in ipairs(bucket_prefixes) do
		local bucket_key = "malware_scan:hashes:" .. prefix
		local count = database.hlen(plugin, bucket_key) or 0
		if count > 0 then
			return true  -- At least one bucket has data
		end
	end

	return false  -- All buckets are empty
end

-- Initialize hashes on startup if they don't exist
-- Called on first scheduler run after BunkerWeb restart
-- Forces an immediate download and import if hashes are missing
--- @param plugin Plugin instance
--- @return boolean success True if initialization succeeded or was skipped, false on error
function scheduler.init_on_startup(plugin)
	-- Check if plugin is enabled
	if plugin.variables["USE_MALWARE_SCANNER"] ~= "yes" then
		return true  -- Plugin disabled, skip
	end

	-- Check if custom hashes feature is enabled
	if plugin.variables["MALWARE_SCAN_CUSTOM_HASHES_ENABLED"] ~= "yes" then
		return true  -- Feature disabled, skip
	end

	-- Check if auto-update is enabled
	if plugin.variables["MALWARE_SCAN_CUSTOM_HASHES_AUTO_UPDATE"] ~= "yes" then
		return true  -- Auto-update disabled, skip
	end

	-- Validate Redis configuration
	local redis_valid, redis_error = scheduler_validate_redis_config(plugin)
	if not redis_valid then
		logger.log_error("[v" .. scheduler.VERSION .. "] [SCHEDULER] [INIT] Redis configuration error: " .. redis_error)
		return false
	end

	-- Check if hashes already exist
	if scheduler_hashes_exist(plugin) then
		logger.log_notice( "[SCHEDULER] Hash buckets already populated - skipping startup initialization")
		return true
	end

	-- Hashes don't exist - force immediate import
	logger.log_notice( "[SCHEDULER] Hash buckets are empty - forcing import on startup")

	-- Clear state file to force fresh download
	local csv_path = plugin.variables["MALWARE_SCAN_CUSTOM_HASHES_CSV_PATH"] or "/var/cache/bunkerweb/malware-scan/hashes.csv"
	local state_file = csv_path:gsub("%.csv$", ".state")

	-- Delete state file to force fresh download (ETag will be different) - native Lua
	os.remove(state_file)  -- Ignore errors - file may not exist
	logger.log_notice( "[SCHEDULER] State file cleared - fresh download will be triggered")

	-- Now trigger the normal update process
	-- This will download and import hashes
	return scheduler.update_hashes(plugin)
end

-- ============================================================================
-- REDIS-BASED FULL DOWNLOAD TRACKING
-- ============================================================================
-- Tracks two Redis keys:
-- 1. malware_scan:last_full_download (22h TTL) - tracks recent full download
-- 2. malware_scan:full_download_ever_done (30h TTL) - tracks if ever downloaded

-- Check if full download is needed based on Redis TTL
--- @param plugin Plugin instance
--- @return boolean needs_download True if full download is needed
--- @return boolean is_first_time True if never downloaded before, false if refreshing
local function scheduler_needs_full_download(plugin)
	if not plugin.redisdb then
		return true, true  -- No Redis, default to allowing full download (treat as first time)
	end

	local full_key = "malware_scan:last_full_download"
	local full_key_ever = "malware_scan:full_download_ever_done"

	local exists = database.exists(plugin, full_key)
	local ever_done = database.exists(plugin, full_key_ever)

	if exists == 0 then
		-- Key doesn't exist or expired (older than 22 hours)
		local is_first_time = (ever_done == 0)
		return true, is_first_time
	end

	-- Key exists and hasn't expired yet
	return false, false
end

-- Record full download timestamp in Redis with 22-hour TTL
-- Also sets a marker that full download has been done at least once
--- @param plugin Plugin instance
local function scheduler_record_full_download(plugin)
	if not plugin.redisdb then
		return
	end

	local full_key = "malware_scan:last_full_download"
	local full_key_ever = "malware_scan:full_download_ever_done"
	local timestamp = os.date("!%Y-%m-%dT%H:%M:%SZ") --[[@as string]]

	-- Set with 22-hour TTL (79200 seconds)
	database.setex(plugin, full_key, 79200, timestamp)

	-- Set marker with 30-hour TTL (108000 seconds) to track if ever done
	database.setex(plugin, full_key_ever, 108000, timestamp)

	logger.log_notice( "[SCHEDULER] Full download recorded in Redis with 22h TTL: " .. timestamp)
end

-- ============================================================================
-- SCHEDULER MAIN FUNCTION
-- ============================================================================

-- Scheduled task for periodic MalwareBazaar hash updates
-- Called by BunkerWeb scheduler at configured intervals
--
-- New behavior:
-- - Run recent files download every 5 minutes for 55 minutes (11 total downloads)
-- - Each download has 1-45 second random delay to distribute server load
-- - After 55 minutes complete, check for full download (based on 22h TTL)
-- - Full download behavior:
--   * First time (never downloaded): immediate download
--   * Refresh (expired after 22h): random delay of 1-14 minutes before download
--
--- @param plugin Plugin instance
--- @return boolean success True on success, false on failure
function scheduler.update_hashes(plugin)
	-- Check if plugin is enabled (auto-update requires main plugin to be enabled)
	if plugin.variables["USE_MALWARE_SCANNER"] ~= "yes" then
		return true  -- Plugin disabled, skip auto-update
	end

	-- Check if auto-update is explicitly enabled
	if plugin.variables["MALWARE_SCAN_CUSTOM_HASHES_AUTO_UPDATE"] ~= "yes" then
		return true  -- Auto-update disabled
	end

	-- Check if custom hashes feature is enabled
	if plugin.variables["MALWARE_SCAN_CUSTOM_HASHES_ENABLED"] ~= "yes" then
		return true  -- Custom hashes disabled
	end

	-- Validate Redis configuration (required for custom hashes)
	local redis_valid, redis_error = scheduler_validate_redis_config(plugin)
	if not redis_valid then
		logger.log_error("[v" .. scheduler.VERSION .. "] [SCHEDULER] [UPDATE] Redis configuration error: " .. redis_error)
		return false
	end

	logger.log_notice("[v" .. scheduler.VERSION .. "] [SCHEDULER] Starting MalwareBazaar hash update")

	-- Get configuration
	local base_url = plugin.variables["MALWARE_SCAN_CUSTOM_HASHES_UPDATE_URL"]
	local csv_path = plugin.variables["MALWARE_SCAN_CUSTOM_HASHES_CSV_PATH"] or "/var/cache/bunkerweb/malware-scan/hashes.csv"
	local state_file = csv_path:gsub("%.csv$", ".state")
	local zip_file = csv_path:gsub("%.csv$", ".zip")

	-- Helper function to download and import a file
	local function download_and_import(url, file_type)
		logger.log_notice( "[SCHEDULER] Checking remote file metadata for " .. file_type .. " update")
		local remote_etag, remote_size, remote_modified = scheduler_get_remote_metadata(url)

		if not remote_etag then
			logger.log_error( "[SCHEDULER] Failed to get remote metadata for " .. file_type)
			return false
		end

		logger.log_notice( "[SCHEDULER] Remote (" .. file_type .. "): ETag=" .. remote_etag .. ", Size=" .. (remote_size or "?"))

		-- Load saved state
		local saved_state = scheduler_load_state(state_file)

		-- Check if file has changed
		if saved_state.saved_etag == remote_etag and saved_state.saved_size == remote_size then
			logger.log_notice( "[SCHEDULER] " .. file_type .. " file unchanged - skipping download")
			return true
		end

		-- File changed - download new version
		logger.log_notice( "[SCHEDULER] " .. file_type .. " file changed - downloading fresh copy")
		if not scheduler_download_file(url, zip_file) then
			logger.log_error( "[SCHEDULER] " .. file_type .. " download failed")
			return false
		end

		logger.log_notice( "[SCHEDULER] " .. file_type .. " download complete")

		-- Extract and import
		logger.log_notice( "[SCHEDULER] Extracting and importing " .. file_type .. " CSV")
		local added, duplicates = scheduler_import_from_zip(zip_file, plugin)

		logger.log_notice( "[SCHEDULER] " .. file_type .. " import complete: added=" .. added .. ", duplicates=" .. duplicates)

		-- Save state with ETag
		local state_to_save = scheduler_load_state(state_file)
		state_to_save.saved_etag = remote_etag
		state_to_save.saved_size = remote_size
		state_to_save.saved_modified = remote_modified

		scheduler_save_state(state_file, state_to_save)

		return true
	end

	-- PHASE 1: Run recent files download every 5 minutes for 55 minutes (11 total downloads)
	-- Downloads occur at: 0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55 minutes
	-- Each download has a random 1-45 second delay to distribute server load
	local recent_url = scheduler_get_download_url(base_url, "recent")
	local total_downloads = 11
	local sleep_interval = 300  -- 5 minutes = 300 seconds

	logger.log_notice( "[SCHEDULER] Phase 1: Starting 55-minute recent files update cycle (" .. total_downloads .. " downloads)")

	for i = 1, total_downloads do
		-- Add random delay (1-45 seconds) to distribute server load
		-- Prevents all instances from hitting server at exactly :00, :05, :10, etc.
		math.randomseed(os.time() + ngx.worker.pid())
		local random_delay = math.random(1, 45)
		logger.log_notice( string.format("[v" .. scheduler.VERSION .. "] [SCHEDULER] Random delay: %d seconds (load distribution)", random_delay))
		ngx.sleep(random_delay)

		logger.log_notice( string.format("[v" .. scheduler.VERSION .. "] [SCHEDULER] Recent download %d/%d (at +%d minutes +%ds)", i, total_downloads, (i-1) * 5, random_delay))

		local success = download_and_import(recent_url, "recent")
		if not success then
			logger.log_error( string.format("[v" .. scheduler.VERSION .. "] [SCHEDULER] Recent download %d/%d failed - continuing", i, total_downloads))
		end

		-- Sleep between downloads (but not after the last one)
		if i < total_downloads then
			logger.log_notice( string.format("[v" .. scheduler.VERSION .. "] [SCHEDULER] Sleeping 5 minutes before next download (completed %d/%d)", i, total_downloads))
			ngx.sleep(sleep_interval)
		end
	end

	logger.log_notice( "[SCHEDULER] Phase 1 complete: 55-minute recent files update cycle finished")

	-- PHASE 2: Check if full download is needed (based on 22-hour TTL)
	local needs_download, is_first_time = scheduler_needs_full_download(plugin)

	if needs_download then
		if is_first_time then
			-- First time download - do it immediately
			logger.log_notice( "[SCHEDULER] Phase 2: Full download needed (never done before) - downloading immediately")
		else
			-- Refresh download - add random delay to distribute server load
			math.randomseed(os.time() + ngx.worker.pid())
			local refresh_delay = math.random(1, 14) * 60  -- 1-14 minutes in seconds
			logger.log_notice( string.format("[v" .. scheduler.VERSION .. "] [SCHEDULER] Phase 2: Full download refresh needed (expired) - random delay: %d minutes", refresh_delay / 60))
			ngx.sleep(refresh_delay)
		end

		local full_url = scheduler_get_download_url(base_url, "full")
		local success = download_and_import(full_url, "full")

		if success then
			-- Record full download timestamp with 22h TTL
			scheduler_record_full_download(plugin)
		else
			logger.log_error( "[SCHEDULER] Full download failed")
		end
	else
		logger.log_notice( "[SCHEDULER] Phase 2: Skipping full download (still fresh, less than 22 hours old)")
	end

	-- Get total count across all 16 buckets
	local total_count = 0
	local bucket_prefixes = {"0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "A", "B", "C", "D", "E", "F"}

	for _, prefix in ipairs(bucket_prefixes) do
		local bucket_key = "malware_scan:hashes:" .. prefix
		local count = database.hlen(plugin, bucket_key) or 0
		total_count = total_count + count
	end

	logger.log_notice( "[SCHEDULER] Total hashes in database: " .. total_count .. " across 16 buckets")

	return true
end

-- ============================================================================
-- COMBINED SCHEDULER WORKFLOW
-- ============================================================================

--- Combined scheduler workflow: Run initialization and hash updates.
--- Convenience function for BunkerWeb scheduler hook to perform all scheduler tasks in one call.
--- Handles both startup initialization and periodic hash updates.
---
--- Example:
---   scheduler.run(plugin)
---
--- @param plugin Plugin instance (provides access to self.logger and self.variables)
--- @return boolean success Always returns true to avoid breaking other scheduled tasks
function scheduler.run(plugin)
	-- Initialize hashes on startup if they don't exist
	-- This runs on first scheduler invocation after BunkerWeb restart
	local init_ok = scheduler.init_on_startup(plugin)
	if not init_ok then
		logger.log_warn( "[SCHEDULER] Startup initialization failed - continuing with regular update")
	end

	-- Run hash update if auto-update is enabled
	local update_ok = scheduler.update_hashes(plugin)
	if not update_ok then
		logger.log_error( "[SCHEDULER] Hash update failed")
	end

	-- Always return true to avoid breaking other scheduled tasks
	return true
end

-- Return module
return scheduler
